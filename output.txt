æ–‡ä»¶å¤¹ 'InferBench-Linux' çš„å±‚çº§ç»“æ„:

â””â”€â”€ InferBench-Linux/
    â”œâ”€â”€ Product_Requirements_Document.md
    â”œâ”€â”€ Code_Quality_Standards.md
    â”œâ”€â”€ Delivery_Standards.md
    â”œâ”€â”€ CMakeLists.txt
    â”œâ”€â”€ output.txt
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ Project_Schedule.md
    â””â”€â”€ include/
        â”œâ”€â”€ InferenceEngine.h
        â”œâ”€â”€ BenchmarkRunner.h
        â”œâ”€â”€ SystemMonitor.h
    â””â”€â”€ tests/
        â”œâ”€â”€ test_inference.cpp
        â”œâ”€â”€ test_monitor.cpp
        â”œâ”€â”€ test_main.cpp
        â”œâ”€â”€ test_benchmark.cpp
        â”œâ”€â”€ resnet50.onnx
    â””â”€â”€ scripts/
        â”œâ”€â”€ setup_deps.sh
    â””â”€â”€ src/
        â”œâ”€â”€ main.cpp
        â”œâ”€â”€ test_ort.cpp
        â”œâ”€â”€ SystemMonitor.cpp
        â”œâ”€â”€ InferenceEngine.cpp
        â”œâ”€â”€ BenchmarkRunner.cpp

================================================================================

--- æ–‡ä»¶å¼€å§‹: Product_Requirements_Document.md ---

# äº§å“éœ€æ±‚æ–‡æ¡£ (PRD): InferBench-Linux
> **ç‰ˆæœ¬**: v0.1.0 (MVP)  
> **æ—¥æœŸ**: 2026-01-17  
> **çŠ¶æ€**: å·²æ‰¹å‡†  

## 1. é¡¹ç›®èƒŒæ™¯
### 1.1 ç—›ç‚¹åˆ†æ
åœ¨ AI ç®—æ³•è½åœ°è¿‡ç¨‹ä¸­ï¼Œé™¤äº†æ¨¡å‹ç²¾åº¦ï¼Œæ¨ç†å»¶è¿Ÿ (Latency)ã€ååé‡ (QPS) ä»¥åŠç³»ç»Ÿèµ„æºå ç”¨ (CPU/Memory) æ˜¯å†³å®šæ¨¡å‹èƒ½å¦ä¸Šçº¿çš„å…³é”®æŒ‡æ ‡ã€‚ç›®å‰ç¼ºä¹ä¸€ä¸ªè½»é‡çº§ã€æ— ä¾èµ–ã€ä¸”èƒ½åŒæ—¶ç›‘æ§ç³»ç»Ÿåº•å±‚æŒ‡æ ‡çš„ C++ åŸç”Ÿæµ‹è¯•å·¥å…·ã€‚

### 1.2 é¡¹ç›®ç›®æ ‡
å¼€å‘ä¸€ä¸ªè¿è¡Œåœ¨ Linux ç»ˆç«¯çš„å‘½ä»¤è¡Œå·¥å…· (CLI) `inferbench`ï¼Œå…·å¤‡ä»¥ä¸‹èƒ½åŠ›ï¼š
- åŠ è½½é€šç”¨çš„ ONNX æ¨¡å‹ï¼ˆé¿å…å¼•å…¥ OpenCV ç­‰å¤æ‚ä¾èµ–ï¼‰ã€‚
- è¿›è¡Œå¤šçº¿ç¨‹é«˜å¹¶å‘æ¨ç†å‹æµ‹ã€‚
- å®æ—¶ç›‘æ§ Linux ç³»ç»Ÿçº§ï¼ˆæ•´ä½“ CPUï¼‰å’Œè¿›ç¨‹çº§ï¼ˆå½“å‰è¿›ç¨‹å†…å­˜ï¼‰çš„èµ„æºæ¶ˆè€—ã€‚
- è¾“å‡ºæ ‡å‡†åŒ–çš„æ€§èƒ½æµ‹è¯•æŠ¥å‘Š (JSON/CSV)ã€‚

## 2. æŠ€æœ¯æ¶æ„
### 2.1 æŠ€æœ¯æ ˆ
- **æ ¸å¿ƒè¯­è¨€**: C++17
- **æ„å»ºç³»ç»Ÿ**: CMake
- **æ¨ç†åç«¯**: ONNX Runtime (C++ API)
- **å¹¶å‘æ¨¡å‹**: `std::thread` + çº¿ç¨‹æ± 
- **ç³»ç»Ÿç›‘æ§**: ç›´æ¥è¯»å– Linux `/proc` æ–‡ä»¶ç³»ç»Ÿ (æ— ç¬¬ä¸‰æ–¹ä¾èµ–)
- **å•å…ƒæµ‹è¯•**: Google Test (GTest)

### 2.2 æ¨¡å—åˆ’åˆ†
1.  **æ¥å…¥å±‚ (Interface Layer)**: è´Ÿè´£è§£æå‘½ä»¤è¡Œå‚æ•° (`--model`, `--threads` ç­‰) åŠæµç¨‹æ§åˆ¶ã€‚
2.  **æ¨ç†å¼•æ“ (InferenceEngine)**: å°è£… ONNX Runtime ä¼šè¯ï¼Œè´Ÿè´£æ¨¡å‹åŠ è½½å’Œ Tensor ç®¡ç†ã€‚
3.  **å‹æµ‹æ ¸å¿ƒ (BenchmarkRunner)**: è´Ÿè´£çº¿ç¨‹æ± ç®¡ç†ã€ä»»åŠ¡è°ƒåº¦å’Œå»¶è¿Ÿè®¡ç®—ã€‚
4.  **ç›‘æ§æ¨¡å— (SystemMonitor)**: åå°çº¿ç¨‹è¯»å– `/proc/stat` å’Œ `/proc/self/stat`ã€‚
5.  **æ•°æ®å±‚ (Data Layer)**: è´Ÿè´£ç»Ÿè®¡æ•°æ®èšåˆåŠæŠ¥å‘Šç”Ÿæˆã€‚

## 3. MVP åŠŸèƒ½éœ€æ±‚

### 3.1 æ¨¡å‹ä¸æ•°æ®
- **æ¨¡å‹åŠ è½½**: æ”¯æŒé€šè¿‡æ–‡ä»¶è·¯å¾„åŠ è½½ `.onnx` æ¨¡å‹ã€‚
- **æ•°æ®è¾“å…¥**:
    - è‡ªåŠ¨æ¢æµ‹è¾“å…¥èŠ‚ç‚¹çš„åç§°å’Œç»´åº¦ã€‚
    - è‡ªåŠ¨ç”Ÿæˆç¬¦åˆ Input Shape çš„éšæœºæµ®ç‚¹å¼ é‡ (Float32)ã€‚
    - *çº¦æŸ*: MVP é˜¶æ®µä¸å¼•å…¥å›¾åƒå¤„ç†åº“ï¼ˆå¦‚ OpenCVï¼‰ã€‚

### 3.2 å‹æµ‹é…ç½®
- **å¹¶å‘æ§åˆ¶**: æ”¯æŒé…ç½®çº¿ç¨‹æ•° (`-t/--threads`)ã€‚
- **è´Ÿè½½æ§åˆ¶**: 
    - å›ºå®šè¯·æ±‚æ€»æ•° (`-n/--requests`)ã€‚
    - *ä¼˜åŒ–é¡¹*: æ”¯æŒé¢„çƒ­è½®æ•° (`--warmup`) ä»¥ç¨³å®šæ€§èƒ½ã€‚

### 3.3 ç³»ç»Ÿç›‘æ§
- **é‡‡æ ·é¢‘ç‡**: å¯é…ç½®ï¼ˆé»˜è®¤å¦‚ 100msï¼‰ã€‚
- **ç›‘æ§æŒ‡æ ‡**:
    - **è¿›ç¨‹å†…å­˜**: RSS (å¸¸é©»å†…å­˜é›†)ã€‚
    - **ç³»ç»Ÿ CPU**: å…¨å±€ CPU ä½¿ç”¨ç‡ç™¾åˆ†æ¯”ã€‚

### 3.4 è¾“å‡ºä¸æŠ¥å‘Š
- **äº¤äº’ä½“éªŒ**: ç»ˆç«¯å®æ—¶æ˜¾ç¤ºè¿›åº¦æ¡ã€‚
- **ç»“æœæ±‡æ€»**:
    - å¹³å‡å»¶è¿Ÿ (Avg Latency)ã€P99 å»¶è¿Ÿã€‚
    - ååé‡ (QPS)ã€‚
    - å³°å€¼å†…å­˜ (Peak Memory RSS)ã€‚
    - å¹³å‡ CPU ä½¿ç”¨ç‡ã€‚
- **æ–‡ä»¶è¾“å‡º**: æ ‡å‡†åŒ– JSON å’Œ CSV æŠ¥å‘Šã€‚

## 4. å·¥ä½œæµç¨‹
1.  **ç”¨æˆ·** å¯åŠ¨å·¥å…·: `./inferbench --model resnet50.onnx --threads 4 --requests 1000`
2.  **å·¥å…·** åˆå§‹åŒ–ç¯å¢ƒ (ONNX Env) -> åŠ è½½æ¨¡å‹ã€‚
3.  **å·¥å…·** æ‰§è¡Œé¢„çƒ­ (Warmup, å¯é€‰)ã€‚
4.  **å·¥å…·** å¯åŠ¨ç›‘æ§çº¿ç¨‹ (Monitor Thread)ã€‚
5.  **å·¥å…·** å¯åŠ¨å·¥ä½œçº¿ç¨‹æ±  (Benchmark Work)ã€‚
6.  **å·¥å…·** èšåˆç»“æœ -> æ‰“å°åˆ°ç»ˆç«¯ -> ä¿å­˜è‡³æ–‡ä»¶ã€‚


--- æ–‡ä»¶ç»“æŸ: Product_Requirements_Document.md ---

--- æ–‡ä»¶å¼€å§‹: Code_Quality_Standards.md ---

# ä»£ç è´¨é‡æŒ‡æ ‡ (Code Quality Standards)

## 1. ä»£ç è§„èŒƒ (Style)
- **é£æ ¼æŒ‡å—**: éµå¾ª [Google C++ Style Guide](https://google.github.io/styleguide/cppguide.html)ã€‚
  - ç±»å: `PascalCase`
  - å‡½æ•°å: `PascalCase`
  - å˜é‡å: `snake_case`
  - ç§æœ‰æˆå‘˜å˜é‡: `snake_case_` (å¸¦å°¾éƒ¨ä¸‹åˆ’çº¿)
  - ç¼©è¿›: 2 æˆ– 4 ç©ºæ ¼ (ä¿æŒä¸€è‡´)
- **æ ¼å¼åŒ–**: å»ºè®®ä½¿ç”¨ `.clang-format` è¿›è¡Œè‡ªåŠ¨æ ¼å¼åŒ–ã€‚

## 2. æ–‡æ¡£æ³¨é‡Š (Documentation)
- **å…¬å…± API**: æ‰€æœ‰ `public` çš„ç±»ã€ç»“æ„ä½“ã€æ–¹æ³•å¿…é¡»åŒ…å« Doxygen é£æ ¼çš„ä¸­æ–‡æ³¨é‡Šã€‚
- **æ ¼å¼è¦æ±‚**:
  - `/** ... */` å—æ³¨é‡Šã€‚
  - ä½¿ç”¨ `@brief` ç®€è¿°åŠŸèƒ½ã€‚
  - ä½¿ç”¨ `@param` æè¿°å‚æ•°ã€‚
  - ä½¿ç”¨ `@return` æè¿°è¿”å›å€¼ã€‚
  - å¤æ‚é€»è¾‘éœ€åœ¨å®ç°ä»£ç ä¸­æ·»åŠ è¡Œå†…æ³¨é‡Š `//`.

## 3. å†…å­˜å®‰å…¨ (Memory Safety)
- **é›¶æ³„æ¼**: å¿…é¡»é€šè¿‡åŠ¨æ€åˆ†æå·¥å…·éªŒè¯ã€‚
- **å·¥å…·**: 
  - CI/CD æµç¨‹ä¸­éœ€å¼€å¯ AddressSanitizer (`-fsanitize=address`)ã€‚
  - æˆ–ä½¿ç”¨ Valgrind Memcheck è¿è¡Œæµ‹è¯•ã€‚

## 4. å¹¶å‘å®‰å…¨ (Concurrency)
- **çº¿ç¨‹å®‰å…¨**: å¤šçº¿ç¨‹å…±äº«çš„å¯å˜çŠ¶æ€å¿…é¡»åŠ é” (`std::mutex`, `std::atomic`)ã€‚
- **æ— æ­»é”**: é¿å…åœ¨æŒé”æœŸé—´è°ƒç”¨å¤–éƒ¨æœªçŸ¥å›è°ƒï¼Œéµå¾ªé”é¡ºã€‚


--- æ–‡ä»¶ç»“æŸ: Code_Quality_Standards.md ---

--- æ–‡ä»¶å¼€å§‹: Delivery_Standards.md ---

# äº¤ä»˜æ ‡å‡†: InferBench-Linux

## 1. ä»£ç è´¨é‡æŒ‡æ ‡ (Code Quality)
> è¯¦è§å•ç‹¬æ–‡æ¡£: [Code_Quality_Standards.md](./Code_Quality_Standards.md)

## 2. åŠŸèƒ½éªŒæ”¶æ ‡å‡† (Acceptance Criteria)
1.  **åŠ è½½æµ‹è¯•**: èƒ½å¤ŸæˆåŠŸåŠ è½½æ ‡å‡†çš„ ONNX æ¨¡å‹ (å¦‚ ResNet50, YOLOv5, BERT-tiny)ã€‚
2.  **ç¨³å®šæ€§æµ‹è¯•**: èƒ½å¤Ÿè¿ç»­è¿è¡Œè‡³å°‘ 1 å°æ—¶è€Œä¸å´©æºƒï¼Œä¸”ä¸å‘ç”Ÿ OOM (Out of Memory)ã€‚
3.  **ç²¾åº¦è¦æ±‚ - å»¶è¿Ÿ**: æµ‹é‡ä»£ç æœ¬èº«çš„è€—æ—¶å¼€é”€ (Overhead) å¿…é¡»å°äºå®é™…æ¨ç†è€—æ—¶çš„ 1% (å¯¹äºè€—æ—¶ >5ms çš„æ¨¡å‹)ã€‚
4.  **ç²¾åº¦è¦æ±‚ - ç›‘æ§**: CPU/å†…å­˜çš„è¯»æ•°åº”ä¸æ ‡å‡†å·¥å…· (å¦‚ `top` æˆ– `htop`) ç›¸æ¯”ï¼Œè¯¯å·®åœ¨ Â±5% ä»¥å†…ã€‚

## 3. äº¤ä»˜ç‰©æ¸…å• (Deliverables)
1.  **å¯æ‰§è¡Œæ–‡ä»¶**: é™æ€é“¾æ¥ï¼ˆæˆ–æ­£ç¡®è®¾ç½® RPATHï¼‰çš„äºŒè¿›åˆ¶æ–‡ä»¶ `inferbench`ã€‚
2.  **è„šæœ¬**: ç”¨äº MVP è‡ªåŠ¨åŒ–éªŒè¯çš„ `run_test.sh` è„šæœ¬ã€‚
3.  **æµ‹è¯•æŠ¥å‘Š**: å‹æµ‹ç»“æœæ ·ä¾‹æ–‡ä»¶ (`benchmark_result.json`)ã€‚
4.  **æ–‡æ¡£**: å®Œæ•´çš„ `README.md`ï¼ŒåŒ…å«ç¼–è¯‘æŒ‡å—å’Œä½¿ç”¨ç¤ºä¾‹ã€‚

## 4. æµ‹è¯•è¦†ç›–ç‡ (Test Coverage)
- **å•å…ƒæµ‹è¯•**: `SystemMonitor` å’Œ `Stats` ç»Ÿè®¡é€»è¾‘çš„æµ‹è¯•è¦†ç›–ç‡éœ€ > 80%ã€‚
- **é›†æˆæµ‹è¯•**: åŒ…å«è‡ªåŠ¨åŒ–è„šæœ¬ï¼Œè·‘é€š Dummy æ¨¡å‹çš„å…¨æµç¨‹ã€‚


--- æ–‡ä»¶ç»“æŸ: Delivery_Standards.md ---

--- æ–‡ä»¶å¼€å§‹: CMakeLists.txt ---

# æŒ‡å®š CMake çš„æœ€ä½ç‰ˆæœ¬è¦æ±‚
cmake_minimum_required(VERSION 3.10)

# è®¾ç½®é¡¹ç›®åç§°ã€ç‰ˆæœ¬å·ä»¥åŠä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼ˆC++ï¼‰
project(InferBench-Linux VERSION 0.1.0 LANGUAGES CXX)

# è®¾ç½® C++ æ ‡å‡†
# æŒ‡å®šä½¿ç”¨ C++17 æ ‡å‡†
set(CMAKE_CXX_STANDARD 17)
# å¼ºåˆ¶è¦æ±‚ç¼–è¯‘å™¨æ”¯æŒæ‰€é€‰çš„ C++ æ ‡å‡†
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# è®¾ç½®ç¼–è¯‘äº§ç‰©çš„è¾“å‡ºç›®å½•
# è®¾ç½®é™æ€åº“ï¼ˆ.aï¼‰çš„è¾“å‡ºç›®å½•
set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
# è®¾ç½®åŠ¨æ€åº“ï¼ˆ.soï¼‰çš„è¾“å‡ºç›®å½•
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
# è®¾ç½®å¯æ‰§è¡ŒäºŒè¿›åˆ¶æ–‡ä»¶çš„è¾“å‡ºç›®å½•
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)

# æŒ‡å®šå¤´æ–‡ä»¶æœç´¢è·¯å¾„
include_directories(include)
include_directories(third_party/onnxruntime/include)

# æŒ‡å®šåº“æ–‡ä»¶æœç´¢è·¯å¾„
link_directories(third_party/onnxruntime/lib)

# --- GoogleTest Integration ---
include(FetchContent)
FetchContent_Declare(
  googletest
  URL https://github.com/google/googletest/archive/refs/tags/v1.14.0.zip
)
# For Windows: Prevent overriding the parent project's compiler/linker settings
set(gtest_force_shared_crt ON CACHE BOOL "" FORCE)
FetchContent_MakeAvailable(googletest)
# -----------------------------

# æŸ¥æ‰¾æ‰€æœ‰æºæ–‡ä»¶ 
# file(GLOB...) åœ¨å·¥ç¨‹å˜å¤§æ—¶ä¸æ˜¯æœ€ä½³å®è·µï¼Œä½† MVP é˜¶æ®µä¸ºäº†æ–¹ä¾¿å…ˆä¿ç•™ï¼Œæˆ–è€…æ‰‹åŠ¨åˆ—å‡º
set(SOURCES
    src/test_ort.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)

# æ·»åŠ å¯æ‰§è¡Œæ–‡ä»¶ (Main App)
add_executable(inferbench 
    src/main.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)
target_link_libraries(inferbench onnxruntime)

# --- Unit Tests ---
enable_testing()
add_executable(unit_tests 
    tests/test_main.cpp
    tests/test_monitor.cpp
    tests/test_inference.cpp
    tests/test_benchmark.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)
target_link_libraries(unit_tests GTest::gtest_main onnxruntime)

include(GoogleTest)
gtest_discover_tests(unit_tests)


--- æ–‡ä»¶ç»“æŸ: CMakeLists.txt ---

--- æ–‡ä»¶å¼€å§‹: output.txt ---



--- æ–‡ä»¶ç»“æŸ: output.txt ---

--- æ–‡ä»¶å¼€å§‹: Project_Schedule.md ---

# é¡¹ç›®è¿›åº¦ä¸è·¯çº¿å›¾: InferBench-Linux

## ç¬¬ä¸€é˜¶æ®µ: MVP å¼€å‘ (ç¬¬ä¸€å‘¨)
**ç›®æ ‡**: å®ç°æ ¸å¿ƒåŠŸèƒ½å¹¶å®ŒæˆåŸºç¡€ CLI å¼€å‘ã€‚

### ğŸ“… é‡Œç¨‹ç¢‘åˆ†è§£
- [x] **M1: é¡¹ç›®æ­å»º (Project Setup)**
    - [x] å»ºç«‹ CMake é¡¹ç›®ç»“æ„
    - [x] Git åˆå§‹åŒ–ä¸å¿½ç•¥æ–‡ä»¶é…ç½®
    - [x] ä¾èµ–é…ç½® (å¼•å…¥ ONNX Runtime, GTest)
- [x] **M2: ç³»ç»Ÿç›‘æ§æ¨¡å— (System Monitor)**
    - [x] å®ç° `SystemMonitor` ç±»
    - [x] å®ç° `/proc/stat` è§£æ (CPU)
    - [x] å®ç° `/proc/self/stat` è§£æ (Memory)
- [x] **M3: æ¨ç†å¼•æ“ (Inference Engine)**
    - [x] å°è£… `Ort::Session`
    - [x] å®ç° `LoadModel` æ¥å£
    - [x] å®ç°éšæœº Tensor ç”Ÿæˆå™¨
    - [x] å®ç° `Run` æ¨ç†æ¥å£
- [x] **M4: å‹æµ‹æ ¸å¿ƒ (Benchmark Core)**
    - [x] å®ç°çº¿ç¨‹æ± é€»è¾‘
    - [x] å®ç° `BenchmarkRunner` è°ƒåº¦å™¨
    - [x] å®ç°é«˜ç²¾åº¦è®¡æ—¶ä¸å»¶è¿Ÿç»Ÿè®¡
- [x] **M5: CLI ä¸é›†æˆ (Integration)**
    - [x] å‘½ä»¤è¡Œå‚æ•°è§£æ
    - [x] æ¨¡å—ç»„è£…ä¸è”è°ƒ
    - [x] ç»ˆç«¯è¿›åº¦æ¡å®ç°
    - [x] æŠ¥å‘Šè¾“å‡ºé€»è¾‘
- [ ] **M6: MVP éªŒè¯ (Verification)**
    - [ ] ResNet50/MobileNet è·‘é€šæµ‹è¯•
    - [ ] å†…å­˜æ³„æ¼æ£€æŸ¥ (Valgrind)

## ç¬¬äºŒé˜¶æ®µ: ä¼˜åŒ–ä¸å¥å£®æ€§ (ç¬¬äºŒå‘¨)
**ç›®æ ‡**: æå‡ç¨³å®šæ€§ï¼Œæ”¯æŒæ›´å¤šåœºæ™¯ï¼Œå¢å¼ºæ˜“ç”¨æ€§ã€‚

- [ ] **æŠ¥å‘Šå¢å¼º**: æ”¯æŒè¯¦ç»†çš„ç›´æ–¹å›¾ç»Ÿè®¡ï¼ŒASCII æ ¼å¼çš„å»¶è¿Ÿåˆ†å¸ƒå›¾ã€‚
- [ ] **çœŸå®æ•°æ®æ”¯æŒ**: æ”¯æŒåŠ è½½å¤–éƒ¨æ•°æ®æ–‡ä»¶ (.bin/.npy) è€Œéä»…éšæœºæ•°æ®ã€‚
- [ ] **æ€§èƒ½åˆ†æ**: é›†æˆç®€å•çš„ Profiling åŠŸèƒ½ (åŒºåˆ† å‰å¤„ç†/æ¨ç†/åå¤„ç† è€—æ—¶)ã€‚

## ç¬¬ä¸‰é˜¶æ®µ: é«˜çº§ç‰¹æ€§ (æœªæ¥è§„åˆ’)
- [ ] **GPU æ”¯æŒ**: å¯ç”¨ CUDA/TensorRT Execution Providersã€‚
- [ ] **æœåŠ¡ç«¯æ¨¡å¼**: æš´éœ² HTTP/gRPC æ¥å£ï¼Œæ”¯æŒè¿œç¨‹è§¦å‘å‹æµ‹ã€‚


--- æ–‡ä»¶ç»“æŸ: Project_Schedule.md ---

--- æ–‡ä»¶å¼€å§‹: include\InferenceEngine.h ---

#pragma once

#include <string>
#include <vector>
#include <memory>
#include <onnxruntime_cxx_api.h>

/**
 * @brief æ¨ç†å¼•æ“ç±»ï¼Œå°è£… ONNX Runtime çš„æ ¸å¿ƒåŠŸèƒ½ã€‚
 * 
 * è´Ÿè´£æ¨¡å‹çš„åŠ è½½ã€Tensor å†…å­˜ç®¡ç†ä»¥åŠæ‰§è¡Œæ¨ç†ã€‚
 */
class InferenceEngine {
public:
    InferenceEngine();
    ~InferenceEngine();

    /**
     * @brief åŠ è½½ ONNX æ¨¡å‹ã€‚
     * 
     * @param model_path æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ (.onnx)ã€‚
     * @throws std::runtime_error å¦‚æœåŠ è½½å¤±è´¥ã€‚
     */
    void LoadModel(const std::string& model_path);

    /**
     * @brief æ‰§è¡Œæ¨ç†ã€‚
     * 
     * @param input_data è¾“å…¥æ•°æ®ï¼ˆå±•å¹³çš„ float æ•°ç»„ï¼‰ã€‚
     * @return std::vector<float> æ¨ç†ç»“æœï¼ˆå±•å¹³çš„ float æ•°ç»„ï¼‰ã€‚
     */
    std::vector<float> Run(const std::vector<float>& input_data);

    /**
     * @brief è·å–æ¨¡å‹éœ€è¦çš„è¾“å…¥ Tensor å…ƒç´ æ€»æ•°ã€‚
     * 
     * ç”¨äºç”Ÿæˆç¬¦åˆå¤§å°çš„éšæœºæµ‹è¯•æ•°æ®ã€‚
     * 
     * @return int64_t å…ƒç´ ä¸ªæ•° (å¦‚ 1x3x224x224 = 150528)ã€‚
     */
    int64_t GetInputSize() const;

private:
    // ONNX Runtime ç¯å¢ƒï¼Œæ•´ä¸ªè¿›ç¨‹é€šå¸¸åªéœ€è¦ä¸€ä¸ª
    Ort::Env env_;
    // ä¼šè¯å¯¹è±¡ï¼Œéçº¿ç¨‹å®‰å…¨ï¼ˆä½†åœ¨ Run æ—¶åªè¯»æ¨¡å‹æ˜¯å®‰å…¨çš„ï¼Œå¦‚æœåŒ…å«çŠ¶æ€åˆ™éœ€æ³¨æ„ï¼‰
    // ä¸ºäº†æ›´å®‰å…¨çš„å¹¶å‘ï¼Œé€šå¸¸ session æ˜¯çº¿ç¨‹å®‰å…¨çš„ï¼Œä½† session options ä¸æ˜¯ã€‚
    std::unique_ptr<Ort::Session> session_;
    
    // æ¨¡å‹çš„å…ƒæ•°æ® (Metadata)
    std::vector<const char*> input_node_names_;
    std::vector<const char*> output_node_names_;
    std::vector<int64_t> input_node_dims_;
    size_t input_tensor_size_ = 0;
};


--- æ–‡ä»¶ç»“æŸ: include\InferenceEngine.h ---

--- æ–‡ä»¶å¼€å§‹: include\BenchmarkRunner.h ---

#pragma once

#include "InferenceEngine.h"
#include "SystemMonitor.h"
#include <vector>
#include <cstdint>

/**
 * @brief å‹æµ‹é…ç½®å‚æ•°
 */
struct BenchmarkConfig {
    int threads = 1;        ///< å¹¶å‘çº¿ç¨‹æ•°
    int requests = 100;     ///< æ€»è¯·æ±‚æ•°
    int warmup_rounds = 10; ///< é¢„çƒ­è½®æ•°ï¼ˆä¸è®¡å…¥ç»Ÿè®¡ï¼‰
};

/**
 * @brief å‹æµ‹ç»“æœç»Ÿè®¡
 */
struct BenchmarkResult {
    double qps = 0.0;            ///< ååé‡ (Queries Per Second)
    double avg_latency_ms = 0.0; ///< å¹³å‡å»¶è¿Ÿ (æ¯«ç§’)
    double p99_latency_ms = 0.0; ///< P99 å»¶è¿Ÿ (æ¯«ç§’)
    double avg_cpu_usage = 0.0;  ///< å¹³å‡ CPU ä½¿ç”¨ç‡ (%)
    double peak_memory_mb = 0.0; ///< å³°å€¼å†…å­˜å ç”¨ (MB)
};

/**
 * @brief å‹æµ‹æ ¸å¿ƒè°ƒåº¦å™¨
 * 
 * è´Ÿè´£ç®¡ç†çº¿ç¨‹æ± ã€åˆ†å‘æ¨ç†ä»»åŠ¡ã€æ”¶é›†æ€§èƒ½æ•°æ®ä»¥åŠæ§åˆ¶ç³»ç»Ÿç›‘æ§ã€‚
 * é‡‡ç”¨â€œæŠ¢å•æ¨¡å¼â€è¿›è¡Œè´Ÿè½½å‡è¡¡ã€‚
 */
class BenchmarkRunner {
public:
    /**
     * @brief æ„é€ å‡½æ•°
     * 
     * @param engine ä¾èµ–çš„æ¨ç†å¼•æ“ (å¼•ç”¨ï¼Œç”Ÿå‘½å‘¨æœŸéœ€é•¿äº Runner)
     * @param monitor ä¾èµ–çš„ç³»ç»Ÿç›‘æ§å™¨ (å¼•ç”¨ï¼Œç”Ÿå‘½å‘¨æœŸéœ€é•¿äº Runner)
     */
    BenchmarkRunner(InferenceEngine& engine, SystemMonitor& monitor);
    ~BenchmarkRunner() = default;

    /**
     * @brief æ‰§è¡Œå‹æµ‹
     * 
     * æ­¤å‡½æ•°æ˜¯é˜»å¡çš„ï¼Œç›´åˆ°æ‰€æœ‰è¯·æ±‚å®Œæˆã€‚
     * 
     * @param config å‹æµ‹é…ç½®
     * @return BenchmarkResult æœ€ç»ˆç»Ÿè®¡ç»“æœ
     */
    BenchmarkResult Run(const BenchmarkConfig& config);

private:
    InferenceEngine& engine_;
    SystemMonitor& monitor_;

    /**
     * @brief è®¡ç®—å»¶è¿Ÿçš„ç™¾åˆ†ä½æ•°
     * 
     * @param latencies æ‰€æœ‰æ ·æœ¬æ•°æ®
     * @param percentile ç™¾åˆ†ä½ (e.g. 0.99)
     * @return double å¯¹åº”çš„å»¶è¿Ÿå€¼
     */
    double CalculatePercentile(std::vector<double>& latencies, double percentile);
};


--- æ–‡ä»¶ç»“æŸ: include\BenchmarkRunner.h ---

--- æ–‡ä»¶å¼€å§‹: include\SystemMonitor.h ---

#pragma once

#include <string>
#include <cstdint>
#include <mutex>

/**
 * @brief è´Ÿè´£ç›‘æ§ç³»ç»Ÿèµ„æºï¼ˆCPU å’Œ å†…å­˜ï¼‰
 * 
 * è¿™æ˜¯ä¸€ä¸ªå·¥å…·ç±»ï¼Œç›´æ¥è¯»å– Linux çš„ /proc æ–‡ä»¶ç³»ç»Ÿã€‚
 */
class SystemMonitor {
public:
    SystemMonitor() = default;
    ~SystemMonitor() = default;

    /**
     * @brief è·å–å½“å‰è¿›ç¨‹çš„ç‰©ç†å†…å­˜å ç”¨ (RSS)
     * @return å ç”¨å†…å­˜å¤§å°ï¼Œå•ä½ï¼šMB
     */
    double GetMemoryUsage();

    /**
     * @brief è·å–ç³»ç»Ÿçš„æ•´ä½“ CPU ä½¿ç”¨ç‡
     * 
     * æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªç¬æ—¶å€¼è®¡ç®—ã€‚
     * ç”±äº /proc/stat å­˜å‚¨çš„æ˜¯ç´¯åŠ å€¼ï¼Œæˆ‘ä»¬éœ€è¦ç»´æŠ¤ä¸Šä¸€æ¬¡çš„è¯»æ•°ï¼Œ
     * è®¡ç®— (Current - Last) çš„å·®å€¼æ¥å¾—å‡ºè¿™æ®µæ—¶é—´å†…çš„ä½¿ç”¨ç‡ã€‚
     * 
     * @return CPU ä½¿ç”¨ç‡ç™¾åˆ†æ¯” (0.0 - 100.0)
     */
    double GetCpuUsage();

private:
    // è®°å½•ä¸Šä¸€æ¬¡è¯»å–çš„ CPU æ—¶é—´ç‰‡æ€»å’Œ
    int64_t last_total_cpu_time_ = 0;
    // è®°å½•ä¸Šä¸€æ¬¡è¯»å–çš„ CPU ç©ºé—²æ—¶é—´
    int64_t last_idle_cpu_time_ = 0;
    // äº’æ–¥é”ï¼Œä¿è¯å¤šçº¿ç¨‹è®¿é—®å®‰å…¨
    std::mutex mutex_;
};


--- æ–‡ä»¶ç»“æŸ: include\SystemMonitor.h ---

--- æ–‡ä»¶å¼€å§‹: tests\test_inference.cpp ---

#include <gtest/gtest.h>
#include "InferenceEngine.h"
#include <numeric>
#include <vector>
#include <random>
#include <fstream>

TEST(InferenceEngineTest, LoadModelThrowsOnMissingFile) {
    InferenceEngine engine;
    EXPECT_THROW(engine.LoadModel("non_existent_model.onnx"), std::runtime_error);
}

TEST(InferenceEngineTest, CanRunResNet50) {
    InferenceEngine engine;
    
    // å°è¯•åŠ è½½åˆšåˆšä¸‹è½½çš„æ¨¡å‹
    // æ³¨æ„ï¼šå¦‚æœç½‘ç»œé—®é¢˜å¯¼è‡´ä¸‹è½½å¤±è´¥ï¼Œè¿™ä¸ªæµ‹è¯•ä¼š failã€‚
    // åœ¨çœŸå® CI ä¸­åº”è¯¥ mock æˆ–è€…ä¿è¯æ–‡ä»¶å­˜åœ¨ã€‚
    std::string model_path = "tests/resnet50.onnx";
    std::ifstream f(model_path.c_str());
    if (!f.good()) {
        GTEST_SKIP() << "Skipping test: resnet50.onnx not found";
    }

    ASSERT_NO_THROW(engine.LoadModel(model_path));

    // è·å–è¾“å…¥å¤§å° (ResNet50 -> 1x3x224x224 = 150528)
    int64_t input_size = engine.GetInputSize();
    EXPECT_GT(input_size, 0);

    // ç”Ÿæˆéšæœºæ•°æ®
    std::vector<float> input_data(input_size);
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dist(0.0f, 1.0f);
    for(auto& val : input_data) val = dist(gen);

    // æ‰§è¡Œæ¨ç†
    std::vector<float> output;
    ASSERT_NO_THROW(output = engine.Run(input_data));

    // éªŒè¯è¾“å‡º (ResNet50 ImageNet 1000 classes)
    EXPECT_EQ(output.size(), 1000);
    
    // ç®€å•çš„æ•°å€¼æ£€æŸ¥
    float sum = std::accumulate(output.begin(), output.end(), 0.0f);
    EXPECT_FALSE(std::isnan(sum));
}


--- æ–‡ä»¶ç»“æŸ: tests\test_inference.cpp ---

--- æ–‡ä»¶å¼€å§‹: tests\test_monitor.cpp ---

#include <gtest/gtest.h>
#include "SystemMonitor.h"
#include <thread>
#include <vector>
#include <cmath>

// è¾…åŠ©å‡½æ•°ï¼šåœ¨æµ‹è¯•ä¸­åˆ¶é€ ä¸€äº› CPU è´Ÿè½½
void burn_cpu(int ms) {
    auto start = std::chrono::high_resolution_clock::now();
    while (std::chrono::duration_cast<std::chrono::milliseconds>(
               std::chrono::high_resolution_clock::now() - start).count() < ms) {
        // ç©ºå¾ªç¯
        double x = 0.0;
        for(int i=0; i<1000; ++i) x += std::sin(i);
    }
}

// è¾…åŠ©å‡½æ•°ï¼šåˆ¶é€ å†…å­˜è´Ÿè½½
void burn_memory(std::vector<char>& buffer, size_t size_mb) {
    buffer.resize(size_mb * 1024 * 1024, 1); // åˆ†é…å†…å­˜å¹¶å¡«å…¥æ•°æ®
}

TEST(SystemMonitorTest, MemoryUsageIsReasonable) {
    SystemMonitor monitor;
    
    // 1. åˆå§‹è¯»æ•°åº”è¯¥å¤§äº 0
    double initial_mem = monitor.GetMemoryUsage();
    EXPECT_GT(initial_mem, 0.0);
    std::cout << "[Info] Initial Memory: " << initial_mem << " MB" << std::endl;

    // 2. åˆ†é… 50MB å†…å­˜
    std::vector<char> dummy;
    burn_memory(dummy, 50);

    // 3. å†æ¬¡è¯»å–ï¼Œåº”è¯¥å¢åŠ äº†çº¦ 50MB
    double after_mem = monitor.GetMemoryUsage();
    EXPECT_GT(after_mem, initial_mem + 40.0); // ç•™ä¸€äº› bufferï¼Œç³»ç»Ÿå¯èƒ½åˆ†é…å¤šäºè¯·æ±‚
    std::cout << "[Info] After Alloc 50MB: " << after_mem << " MB" << std::endl;
}

TEST(SystemMonitorTest, CpuUsageIsDynamic) {
    SystemMonitor monitor;

    // ç¬¬ä¸€æ¬¡è°ƒç”¨é€šå¸¸è¿”å› 0 æˆ–ä¸å‡†ç¡®ï¼Œç”¨æ¥åˆå§‹åŒ–åŸºå‡†
    double v1 = monitor.GetCpuUsage();
    
    // ç¡ 200ms
    std::this_thread::sleep_for(std::chrono::milliseconds(200));

    // ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œåº”è¯¥åæ˜ ä¼‘æ¯æ—¶çš„æƒ…å†µï¼ˆæ¥è¿‘ 0ï¼Œä½†ä¸ä¸€å®šæ˜¯ 0ï¼Œå¯èƒ½æœ‰åå°è¿›ç¨‹ï¼‰
    double v2 = monitor.GetCpuUsage();
    std::cout << "[Info] Idle CPU Usage: " << v2 << "%" << std::endl;
    EXPECT_GE(v2, 0.0);
    EXPECT_LE(v2, 100.0);
}


--- æ–‡ä»¶ç»“æŸ: tests\test_monitor.cpp ---

--- æ–‡ä»¶å¼€å§‹: tests\test_main.cpp ---

#include <gtest/gtest.h>

TEST(EnvironmentTest, GTestLinkage) {
    EXPECT_EQ(1 + 1, 2);
}


--- æ–‡ä»¶ç»“æŸ: tests\test_main.cpp ---

--- æ–‡ä»¶å¼€å§‹: tests\test_benchmark.cpp ---

#include <gtest/gtest.h>
#include "BenchmarkRunner.h"
#include <iostream>
#include <fstream>

// è¿™ä¸ªæµ‹è¯•ä¼šçœŸæ­£è·‘èµ·æ¥ï¼Œè™½ç„¶æ˜¯ç”¨éšæœºæ•°æ®ã€‚
// å®ƒéªŒè¯äº†æ‰€æœ‰æ¨¡å—çš„ååŒå·¥ä½œã€‚
TEST(BenchmarkRunnerTest, Integration) {
    // 1. å‡†å¤‡ä¾èµ–
    SystemMonitor monitor;
    InferenceEngine engine;
    
    // å°è¯•åŠ è½½ ResNet50 (å¦‚æœå­˜åœ¨)
    std::string model_path = "tests/resnet50.onnx";
    std::ifstream f(model_path.c_str());
    if (!f.good()) {
        GTEST_SKIP() << "Skipping integration test: model not found";
    }
    
    ASSERT_NO_THROW(engine.LoadModel(model_path));

    // 2. é…ç½®å‹æµ‹
    BenchmarkConfig config;
    config.threads = 4;
    config.requests = 20; // è·‘ 20 æ¬¡æ„æ€ä¸€ä¸‹
    config.warmup_rounds = 2; // çƒ­èº« 2 æ¬¡

    // 3. è¿è¡Œ
    BenchmarkRunner runner(engine, monitor);
    BenchmarkResult result = runner.Run(config);

    // 4. éªŒè¯ç»“æœåˆç†æ€§
    std::cout << "Integration Test Results:" << std::endl;
    std::cout << "QPS: " << result.qps << std::endl;
    std::cout << "Avg Latency: " << result.avg_latency_ms << " ms" << std::endl;
    std::cout << "Avg CPU: " << result.avg_cpu_usage << "%" << std::endl;
    std::cout << "Peak Mem: " << result.peak_memory_mb << " MB" << std::endl;

    EXPECT_GT(result.qps, 0.0);
    EXPECT_GT(result.avg_latency_ms, 0.0);
    // æ³¨æ„ï¼šåœ¨çŸ­æµ‹è¯•é‡Œï¼ŒCPU é‡‡æ ·å¯èƒ½ä¼šé”™è¿‡å³°å€¼æˆ–ä¸º 0ï¼Œæ‰€ä»¥å¾ˆéš¾æ–­è¨€ > 0ï¼Œä½†ä¸åº”å´©å
    EXPECT_GE(result.peak_memory_mb, 0.0);
}


--- æ–‡ä»¶ç»“æŸ: tests\test_benchmark.cpp ---

--- æ–‡ä»¶å¼€å§‹: scripts\setup_deps.sh ---

#!/bin/bash
set -e

# å®šä¹‰ç‰ˆæœ¬å’Œä¸‹è½½é“¾æ¥
ORT_VERSION="1.16.3"
ORT_FILE="onnxruntime-linux-x64-${ORT_VERSION}.tgz"
ORT_URL="https://github.com/microsoft/onnxruntime/releases/download/v${ORT_VERSION}/${ORT_FILE}"

#ä»¥æ­¤è„šæœ¬æ‰€åœ¨ç›®å½•ä¸ºåŸºå‡†ï¼Œæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
SCRIPT_DIR="$( cd "$( dirname "${BASH_SOURCE[0]}" )" &> /dev/null && pwd )"
PROJECT_ROOT="${SCRIPT_DIR}/.."
THIRD_PARTY_DIR="${PROJECT_ROOT}/third_party"

mkdir -p "${THIRD_PARTY_DIR}"

# æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨
if [ -d "${THIRD_PARTY_DIR}/onnxruntime-linux-x64-${ORT_VERSION}" ]; then
    echo "ONNX Runtime ${ORT_VERSION} already exists."
    exit 0
fi

echo "Downloading ONNX Runtime ${ORT_VERSION}..."
wget -q --show-progress "${ORT_URL}" -O "${THIRD_PARTY_DIR}/${ORT_FILE}"

echo "Extracting..."
tar -xzf "${THIRD_PARTY_DIR}/${ORT_FILE}" -C "${THIRD_PARTY_DIR}"

# æ¸…ç†å‹ç¼©åŒ…
rm "${THIRD_PARTY_DIR}/${ORT_FILE}"

# åˆ›å»ºä¸€ä¸ªç¬¦å·é“¾æ¥æ–¹ä¾¿å¼•ç”¨ (onnxruntime -> onnxruntime-linux-x64-1.16.3)
# è¿™æ ·ä»¥åå‡çº§ç‰ˆæœ¬åªéœ€è¦æ”¹é“¾æ¥ï¼Œä¸ç”¨æ”¹ CMakeLists.txt
ln -sfn "${THIRD_PARTY_DIR}/onnxruntime-linux-x64-${ORT_VERSION}" "${THIRD_PARTY_DIR}/onnxruntime"

echo "Success! ONNX Runtime is installed in ${THIRD_PARTY_DIR}/onnxruntime"


--- æ–‡ä»¶ç»“æŸ: scripts\setup_deps.sh ---

--- æ–‡ä»¶å¼€å§‹: src\main.cpp ---

#include <iostream>
#include <string>
#include <vector>
#include <fstream>
#include <getopt.h>
#include <iomanip>

#include "InferenceEngine.h"
#include "SystemMonitor.h"
#include "BenchmarkRunner.h"

// æ‰“å°ä½¿ç”¨è¯´æ˜
void PrintUsage(const char* name) {
    std::cout << "Usage: " << name << " [OPTIONS]\n"
              << "Options:\n"
              << "  -m, --model <path>      Path to ONNX model file (Required)\n"
              << "  -t, --threads <num>     Number of threads (Default: 1)\n"
              << "  -n, --requests <num>    Total number of requests (Default: 100)\n"
              << "  -w, --warmup <num>      Warmup rounds (Default: 10)\n"
              << "  -j, --json <path>       Save report to JSON file\n"
              << "  -h, --help              Show this help message\n";
}

int main(int argc, char** argv) {
    // é»˜è®¤é…ç½®
    std::string model_path;
    std::string json_path;
    BenchmarkConfig config;

    // è§£æå‘½ä»¤è¡Œå‚æ•°
    struct option long_options[] = {
        {"model", required_argument, 0, 'm'},
        {"threads", required_argument, 0, 't'},
        {"requests", required_argument, 0, 'n'},
        {"warmup", required_argument, 0, 'w'},
        {"json", required_argument, 0, 'j'},
        {"help", no_argument, 0, 'h'},
        {0, 0, 0, 0}
    };

    int opt;
    int option_index = 0;
    while ((opt = getopt_long(argc, argv, "m:t:n:w:j:h", long_options, &option_index)) != -1) {
        switch (opt) {
            case 'm': model_path = optarg; break;
            case 't': config.threads = std::stoi(optarg); break;
            case 'n': config.requests = std::stoi(optarg); break;
            case 'w': config.warmup_rounds = std::stoi(optarg); break;
            case 'j': json_path = optarg; break;
            case 'h': PrintUsage(argv[0]); return 0;
            default: PrintUsage(argv[0]); return 1;
        }
    }

    // æ ¡éªŒå¿…å¡«å‚æ•°
    if (model_path.empty()) {
        std::cerr << "Error: --model argument is required.\n";
        PrintUsage(argv[0]);
        return 1;
    }

    std::cout << "========================================" << std::endl;
    std::cout << " InferBench-Linux v0.1.0 (MVP) " << std::endl;
    std::cout << "========================================" << std::endl;
    std::cout << "Model: " << model_path << std::endl;
    std::cout << "Threads: " << config.threads << std::endl;
    std::cout << "Requests: " << config.requests << std::endl;
    std::cout << "Warmup: " << config.warmup_rounds << std::endl;
    std::cout << "----------------------------------------" << std::endl;

    try {
        // 1. åˆå§‹åŒ–æ¨¡å—
        std::cout << "[Init] Initializing Modules..." << std::endl;
        SystemMonitor monitor;
        InferenceEngine engine;

        // 2. åŠ è½½æ¨¡å‹
        std::cout << "[Init] Loading Model..." << std::endl;
        engine.LoadModel(model_path);

        // 3. æ‰§è¡Œå‹æµ‹
        std::cout << "[Run] Starting Benchmark..." << std::endl;
        BenchmarkRunner runner(engine, monitor);
        BenchmarkResult result = runner.Run(config);

        // 4. è¾“å‡ºæŠ¥å‘Š
        std::cout << "----------------------------------------" << std::endl;
        std::cout << " Benchmark Results " << std::endl;
        std::cout << "----------------------------------------" << std::endl;
        std::cout << std::fixed << std::setprecision(2);
        std::cout << "QPS:            " << result.qps << std::endl;
        std::cout << "Avg Latency:    " << result.avg_latency_ms << " ms" << std::endl;
        std::cout << "P99 Latency:    " << result.p99_latency_ms << " ms" << std::endl;
        std::cout << "Avg CPU Usage:  " << result.avg_cpu_usage << " %" << std::endl;
        std::cout << "Peak Memory:    " << result.peak_memory_mb << " MB" << std::endl;
        std::cout << "========================================" << std::endl;

        // 5. ä¿å­˜ JSON
        if (!json_path.empty()) {
            std::ofstream json_file(json_path);
            if (json_file.is_open()) {
                json_file << "{\n";
                json_file << "  \"model\": \"" << model_path << "\",\n";
                json_file << "  \"config\": {\n";
                json_file << "    \"threads\": " << config.threads << ",\n";
                json_file << "    \"requests\": " << config.requests << "\n";
                json_file << "  },\n";
                json_file << "  \"result\": {\n";
                json_file << "    \"qps\": " << result.qps << ",\n";
                json_file << "    \"avg_latency_ms\": " << result.avg_latency_ms << ",\n";
                json_file << "    \"p99_latency_ms\": " << result.p99_latency_ms << ",\n";
                json_file << "    \"avg_cpu_usage\": " << result.avg_cpu_usage << ",\n";
                json_file << "    \"peak_memory_mb\": " << result.peak_memory_mb << "\n";
                json_file << "  }\n";
                json_file << "}\n";
                std::cout << "[Report] Saved to " << json_path << std::endl;
            } else {
                std::cerr << "[Error] Failed to save JSON report." << std::endl;
            }
        }

    } catch (const std::exception& e) {
        std::cerr << "\n[Fatal Error] " << e.what() << std::endl;
        return 1;
    }

    return 0;
}


--- æ–‡ä»¶ç»“æŸ: src\main.cpp ---

--- æ–‡ä»¶å¼€å§‹: src\test_ort.cpp ---

#include <iostream>
#include <onnxruntime_cxx_api.h>

int main() {
    std::cout << "ONNX Runtime Version: " << Ort::GetVersionString() << std::endl;
    return 0;
}


--- æ–‡ä»¶ç»“æŸ: src\test_ort.cpp ---

--- æ–‡ä»¶å¼€å§‹: src\SystemMonitor.cpp ---

#include "SystemMonitor.h"
#include <fstream>
#include <sstream>
#include <iostream>
#include <unistd.h>

double SystemMonitor::GetMemoryUsage() {
    // æ‰“å¼€å½“å‰è¿›ç¨‹çš„çŠ¶æ€æ–‡ä»¶ /proc/self/stat
    std::ifstream stat_file("/proc/self/stat");
    if (!stat_file.is_open()) {
        return 0.0;
    }

    std::string field;
    // /proc/[pid]/stat æ–‡ä»¶ä¸­ç¬¬ 24 ä¸ªå­—æ®µæ˜¯ rss (Resident Set Size)
    // å®ƒè¡¨ç¤ºè¿›ç¨‹å½“å‰å ç”¨çš„ç‰©ç†å†…å­˜é¡µæ•°ã€‚æˆ‘ä»¬éœ€è¦è·³è¿‡å‰ 23 ä¸ªå­—æ®µã€‚
    for (int i = 0; i < 24; ++i) {
        if (!(stat_file >> field)) {
            return 0.0;
        }
    }

    // å°†ç¬¬ 24 ä¸ªå­—æ®µï¼ˆrss é¡µæ•°ï¼‰è½¬æ¢ä¸ºé•¿æ•´å‹
    long rss = std::stol(field);
    // è·å–ç³»ç»Ÿçš„å†…å­˜é¡µå¤§å°ï¼ˆé€šè¿‡ sysconf è·å–ï¼Œé€šå¸¸ä¸º 4096 å­—èŠ‚ï¼‰
    long page_size = sysconf(_SC_PAGESIZE);
    
    // è®¡ç®—æ€»å†…å­˜ä½¿ç”¨é‡ï¼ˆå­—èŠ‚ï¼‰å¹¶è½¬æ¢ä¸º MB
    // è®¡ç®—å…¬å¼ï¼š(é¡µæ•° * æ¯é¡µå­—èŠ‚æ•°) / (1024 * 1024)
    return (static_cast<double>(rss) * page_size) / (1024.0 * 1024.0);
}

double SystemMonitor::GetCpuUsage() {
    // åŠ é”ï¼Œé˜²æ­¢å¤šçº¿ç¨‹åŒæ—¶ä¿®æ”¹ last_total_cpu_time_ ç­‰çŠ¶æ€
    std::lock_guard<std::mutex> lock(mutex_);

    // 1. æ‰“å¼€ /proc/stat æ–‡ä»¶ï¼Œè¿™æ˜¯å…¨å±€ CPU çŠ¶æ€
    std::ifstream stat_file("/proc/stat");
    if (!stat_file.is_open()) {
        return 0.0;
    }

    std::string line;
    std::getline(stat_file, line); // è¯»å–ç¬¬ä¸€è¡Œï¼Œé€šå¸¸ä»¥ "cpu " å¼€å¤´ï¼Œä»£è¡¨æ€» CPU

    std::istringstream iss(line);
    std::string cpu_label;
    iss >> cpu_label; // è·³è¿‡ "cpu" å­—ç¬¦ä¸²

    // 2. è¯»å–å„ä¸ªæ—¶é—´ç‰‡æ•°æ®
    // Linux å†…æ ¸æ–‡æ¡£å®šä¹‰äº†è¿™äº›å­—æ®µé¡ºåºï¼šuser, nice, system, idle, iowait, irq, softirq, steal, guest, guest_nice
    int64_t user, nice, system, idle, iowait, irq, softirq, steal;
    
    // åªè¯»å–å‰ 8 ä¸ªå­—æ®µé€šå¸¸è¶³å¤Ÿè®¡ç®—
    if (!(iss >> user >> nice >> system >> idle >> iowait >> irq >> softirq >> steal)) {
        return 0.0;
    }

    // 3. è®¡ç®— Total å’Œ Idle
    // Idle æ—¶é—´ = idle + iowait
    // Total æ—¶é—´ = user + nice + system + idle + iowait + irq + softirq + steal
    int64_t current_idle = idle + iowait;
    int64_t current_total = user + nice + system + idle + iowait + irq + softirq + steal;

    // 4. è®¡ç®—ä¸ä¸Šä¸€æ¬¡è¯»æ•°çš„å·®å€¼ (Delta)
    int64_t total_delta = current_total - last_total_cpu_time_;
    int64_t idle_delta = current_idle - last_idle_cpu_time_;

    // 5. æ›´æ–°çŠ¶æ€ä»¥å¤‡ä¸‹æ¬¡ä½¿ç”¨
    last_total_cpu_time_ = current_total;
    last_idle_cpu_time_ = current_idle;

    // é˜²æ­¢é™¤ä»¥é›¶ï¼ˆé€šå¸¸ä¹Ÿæ˜¯å¤„ç†ç¬¬ä¸€æ¬¡è¿è¡Œçš„æƒ…å†µï¼‰
    if (total_delta <= 0) {
        return 0.0;
    }

    // 6. è®¡ç®—ä½¿ç”¨ç‡
    // è¿™é‡Œçš„é€»è¾‘åå‘æ€è€ƒï¼šä½¿ç”¨ç‡ = 1.0 - ç©ºé—²ç‡
    double usage = 100.0 * (1.0 - static_cast<double>(idle_delta) / static_cast<double>(total_delta));
    return usage < 0 ? 0 : usage;
}


--- æ–‡ä»¶ç»“æŸ: src\SystemMonitor.cpp ---

--- æ–‡ä»¶å¼€å§‹: src\InferenceEngine.cpp ---

#include "InferenceEngine.h"
#include <iostream>
#include <numeric>
#include <stdexcept>
#include <vector>
#include <cstring>

InferenceEngine::InferenceEngine() 
    : env_(ORT_LOGGING_LEVEL_WARNING, "InferBench") {
}

InferenceEngine::~InferenceEngine() {
    for (auto name : input_node_names_) {
        free(const_cast<char*>(name));
    }
    for (auto name : output_node_names_) {
        free(const_cast<char*>(name));
    }
}

void InferenceEngine::LoadModel(const std::string& model_path) {
    Ort::SessionOptions session_options;
    session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);

    try {
        session_ = std::make_unique<Ort::Session>(env_, model_path.c_str(), session_options);
    } catch (const Ort::Exception& e) {
        throw std::runtime_error("Failed to load model: " + std::string(e.what()));
    }

    Ort::AllocatorWithDefaultOptions allocator;

    // å¤„ç†è¾“å…¥èŠ‚ç‚¹
    size_t num_input_nodes = session_->GetInputCount();
    
    for(size_t i = 0; i < num_input_nodes; i++) {
        auto input_name_ptr = session_->GetInputNameAllocated(i, allocator);
        // æ·±æ‹·è´ nameï¼Œå› ä¸º API è¿”å›çš„ smart pointer ä¼šè‡ªåŠ¨é‡Šæ”¾ï¼Œè€Œæˆ‘ä»¬éœ€è¦åœ¨ Run ä¸­é•¿æœŸæŒæœ‰
        input_node_names_.push_back(strdup(input_name_ptr.get()));

        Ort::TypeInfo type_info = session_->GetInputTypeInfo(i);
        auto tensor_info = type_info.GetTensorTypeAndShapeInfo();
        
        input_node_dims_ = tensor_info.GetShape();
        
        // å¤„ç†åŠ¨æ€ç»´åº¦ (Batch Size = -1)ï¼Œå¼ºåˆ¶è®¾ä¸º 1
        for (auto& dim : input_node_dims_) {
            if (dim < 1) dim = 1;
        }

        input_tensor_size_ = 1;
        for (auto dim : input_node_dims_) {
            input_tensor_size_ *= dim;
        }
    }

    // å¤„ç†è¾“å‡ºèŠ‚ç‚¹
    size_t num_output_nodes = session_->GetOutputCount();
    for(size_t i = 0; i < num_output_nodes; i++) {
        auto output_name_ptr = session_->GetOutputNameAllocated(i, allocator);
        output_node_names_.push_back(strdup(output_name_ptr.get()));
    }
}

std::vector<float> InferenceEngine::Run(const std::vector<float>& input_data) {
    if (input_data.size() != input_tensor_size_) {
        throw std::runtime_error("Input data size mismatch!");
    }

    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

    // Zero-Copy åˆ›å»º input tensor
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, 
        const_cast<float*>(input_data.data()), 
        input_data.size(), 
        input_node_dims_.data(), 
        input_node_dims_.size()
    );

    auto output_tensors = session_->Run(
        Ort::RunOptions{nullptr}, 
        input_node_names_.data(), 
        &input_tensor, 
        1,
        output_node_names_.data(), 
        1
    );

    float* floatarr = output_tensors[0].GetTensorMutableData<float>();
    auto output_tensor_info = output_tensors[0].GetTensorTypeAndShapeInfo();
    size_t output_size = output_tensor_info.GetElementCount();

    return std::vector<float>(floatarr, floatarr + output_size);
}

int64_t InferenceEngine::GetInputSize() const {
    return static_cast<int64_t>(input_tensor_size_);
}


--- æ–‡ä»¶ç»“æŸ: src\InferenceEngine.cpp ---

--- æ–‡ä»¶å¼€å§‹: src\BenchmarkRunner.cpp ---

#include "BenchmarkRunner.h"
#include <algorithm>
#include <atomic>
#include <chrono>
#include <iostream>
#include <random>
#include <thread>
#include <mutex>
#include <numeric>

BenchmarkRunner::BenchmarkRunner(InferenceEngine& engine, SystemMonitor& monitor)
    : engine_(engine), monitor_(monitor) {}

BenchmarkResult BenchmarkRunner::Run(const BenchmarkConfig& config) {
    BenchmarkResult result;

    // 1. å‡†å¤‡æµ‹è¯•æ•°æ® (Random Input)
    int64_t input_size = engine_.GetInputSize();
    std::vector<float> input_data(input_size);
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dist(0.0f, 1.0f);
    for (auto& val : input_data) val = dist(gen);

    // 2. é¢„çƒ­ (Warmup)
    // ç›®çš„ï¼šè®© CPU caches çƒ­èµ·æ¥ & è§¦å‘ ONNX Runtime å†…éƒ¨å¯èƒ½çš„ JIT/Allocations
    for (int i = 0; i < config.warmup_rounds; ++i) {
        engine_.Run(input_data);
    }

    // 3. å‡†å¤‡å¹¶å‘æ§åˆ¶
    std::atomic<int> remaining_requests(config.requests);
    std::vector<std::thread> threads;
    std::mutex stats_mutex; // ä»…ç”¨äºæœ€ååˆå¹¶æ•°æ®
    
    // Per-thread statistics to avoid lock verify
    // è¿™ç§åšæ³•å« TLS (Thread Local Storage) æ€æƒ³çš„æ‰‹åŠ¨å®ç°
    std::vector<std::vector<double>> all_thread_latencies(config.threads);

    // 4. å¯åŠ¨ç³»ç»Ÿç›‘æ§çº¿ç¨‹
    std::atomic<bool> monitor_running(true);
    std::vector<double> cpu_samples;
    std::vector<double> mem_samples;
    
    std::thread monitor_thread([&]() {
        while (monitor_running) {
            cpu_samples.push_back(monitor_.GetCpuUsage());
            mem_samples.push_back(monitor_.GetMemoryUsage());
            std::this_thread::sleep_for(std::chrono::milliseconds(100));
        }
    });

    // 5. å¯åŠ¨ Worker çº¿ç¨‹
    auto start_time = std::chrono::high_resolution_clock::now();

    for (int t = 0; t < config.threads; ++t) {
        threads.emplace_back([&, t]() {
            while (true) {
                // æŠ¢å•ï¼šåŸå­å‡
                // fetch_sub è¿”å›ä¿®æ”¹å‰çš„å€¼
                int current_req_idx = remaining_requests.fetch_sub(1);
                if (current_req_idx <= 0) {
                    break; // æŠ¢æ²¡äº†ï¼Œä¸‹ç­
                }

                auto t1 = std::chrono::high_resolution_clock::now();
                engine_.Run(input_data);
                auto t2 = std::chrono::high_resolution_clock::now();

                // è®°å½•å»¶è¿Ÿ (æ¯«ç§’)
                double lat_ms = std::chrono::duration<double, std::milli>(t2 - t1).count();
                all_thread_latencies[t].push_back(lat_ms);
            }
        });
    }

    // 6. ç­‰å¾…æ‰€æœ‰ Worker ç»“æŸ
    for (auto& t : threads) {
        if (t.joinable()) t.join();
    }

    auto end_time = std::chrono::high_resolution_clock::now();
    double total_time_sec = std::chrono::duration<double>(end_time - start_time).count();

    // 7. åœæ­¢ç›‘æ§
    monitor_running = false;
    if (monitor_thread.joinable()) monitor_thread.join();

    // 8. æ±‡æ€»æ•°æ®
    // åˆå¹¶ Latency
    std::vector<double> flat_latencies;
    flat_latencies.reserve(config.requests);
    for (const auto& local_lats : all_thread_latencies) {
        flat_latencies.insert(flat_latencies.end(), local_lats.begin(), local_lats.end());
    }

    // è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
    result.qps = config.requests / total_time_sec;
    
    if (!flat_latencies.empty()) {
        double sum = std::accumulate(flat_latencies.begin(), flat_latencies.end(), 0.0);
        result.avg_latency_ms = sum / flat_latencies.size();
        result.p99_latency_ms = CalculatePercentile(flat_latencies, 0.99);
    }

    if (!cpu_samples.empty()) {
        double sum = std::accumulate(cpu_samples.begin(), cpu_samples.end(), 0.0);
        result.avg_cpu_usage = sum / cpu_samples.size();
    }

    if (!mem_samples.empty()) {
        // å³°å€¼å†…å­˜
        result.peak_memory_mb = *std::max_element(mem_samples.begin(), mem_samples.end());
    }

    return result;
}

double BenchmarkRunner::CalculatePercentile(std::vector<double>& latencies, double percentile) {
    if (latencies.empty()) return 0.0;
    
    // æ’åºä»¥æ‰¾åˆ° P99
    std::sort(latencies.begin(), latencies.end());
    
    // index = ceil(p / 100 * N) - 1
    // æˆ–ç®€å•çš„ index = p * N
    size_t index = static_cast<size_t>(percentile * latencies.size());
    
    // è¾¹ç•Œä¿æŠ¤
    if (index >= latencies.size()) index = latencies.size() - 1;
    
    return latencies[index];
}


--- æ–‡ä»¶ç»“æŸ: src\BenchmarkRunner.cpp ---

