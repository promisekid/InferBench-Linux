=== Project Structure ===
./
    Code_Quality_Standards.md
    Delivery_Standards.md
    CMakeLists.txt
    .gitignore
    task.md
    README.md
    Project_Schedule.md
    implementation_plan.md
    cmake/
        arm-linux.toolchain.cmake
    include/
        InferenceEngine.h
        BenchmarkRunner.h
        SystemMonitor.h
    tests/
        test_inference.cpp
        test_monitor.cpp
        test_main.cpp
        test_benchmark.cpp
        test_watchdog.cpp
        resnet50.onnx
    .github/
        workflows/
            ci.yml
    scripts/
        setup_deps.sh
        mem_check.sh
        benchmark_suite.py
    src/
        main.cpp
        test_ort.cpp
        SystemMonitor.cpp
        InferenceEngine.cpp
        BenchmarkRunner.cpp


=== Core Source Code Contents ===

==================================================
File: CMakeLists.txt
==================================================
# 指定 CMake 的最低版本要求
cmake_minimum_required(VERSION 3.10)

# 设置项目名称、版本号以及使用的编程语言（C++）
project(InferBench-Linux VERSION 0.1.0 LANGUAGES CXX)

# 设置 C++ 标准
# 指定使用 C++17 标准
set(CMAKE_CXX_STANDARD 17)
# 强制要求编译器支持所选的 C++ 标准
set(CMAKE_CXX_STANDARD_REQUIRED ON)

# --- AddressSanitizer (ASan) Configuration ---
option(ENABLE_ASAN "Enable AddressSanitizer" OFF)
if(ENABLE_ASAN)
    message(STATUS "AddressSanitizer Enabled")
    add_compile_options(-fsanitize=address -fno-omit-frame-pointer -g)
    add_link_options(-fsanitize=address)
endif()
# ---------------------------------------------

# 设置编译产物的输出目录
# 设置静态库（.a）的输出目录
set(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
# 设置动态库（.so）的输出目录
set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)
# 设置可执行二进制文件的输出目录
set(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)

# 指定头文件搜索路径
include_directories(include)
include_directories(third_party/onnxruntime/include)

# 指定库文件搜索路径
link_directories(third_party/onnxruntime/lib)

# --- GoogleTest Integration ---
include(FetchContent)
FetchContent_Declare(
  googletest
  URL https://github.com/google/googletest/archive/refs/tags/v1.14.0.zip
)
# For Windows: Prevent overriding the parent project's compiler/linker settings
set(gtest_force_shared_crt ON CACHE BOOL "" FORCE)
FetchContent_MakeAvailable(googletest)
# -----------------------------

# 查找所有源文件 
set(SOURCES
    src/test_ort.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)

# 添加可执行文件 (Main App)
add_executable(inferbench 
    src/main.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)
target_link_libraries(inferbench onnxruntime)

# --- Unit Tests ---
enable_testing()
add_executable(unit_tests 
    tests/test_main.cpp
    tests/test_monitor.cpp
    tests/test_watchdog.cpp
    tests/test_inference.cpp
    tests/test_benchmark.cpp
    src/SystemMonitor.cpp
    src/InferenceEngine.cpp
    src/BenchmarkRunner.cpp
)
target_link_libraries(unit_tests GTest::gtest_main onnxruntime)

include(GoogleTest)
gtest_discover_tests(unit_tests)


==================================================
File: src/main.cpp
==================================================
#include <iostream>
#include <string>
#include <vector>
#include <fstream>
#include <getopt.h>
#include <iomanip>

#include "InferenceEngine.h"
#include "SystemMonitor.h"
#include "BenchmarkRunner.h"

// 打印模型元数据
void PrintModelInfo(InferenceEngine& engine) {
    std::cout << "[Probe] Model Inspector:\n";
    // 打印输入 Tensor 的基本信息
    std::cout << "  Input Size (elements): " << engine.GetInputSize() << "\n";
    std::cout << "  (More details would require InferenceEngine API expansion)\n";
}

// 打印使用说明
void PrintUsage(const char* name) {
    std::cout << "Usage: " << name << " [OPTIONS]\n"
              << "Options:\n"
              << "  -m, --model <path>      Path to ONNX model file (Required)\n"
              << "  -t, --threads <num>     Number of threads (Default: 1)\n"
              << "  -n, --requests <num>    Total number of requests (Default: 100)\n"
              << "  -w, --warmup <num>      Warmup rounds (Default: 10)\n"
              << "  -l, --memory_limit <MB> Memory Limit in MB (Default: 0, no limit)\n"
              << "  -o, --optimization <lvl> Optimization level: basic, all, none (Default: all)\n"
              << "  --probe                 Print model metadata and exit\n"
              << "  -j, --json <path>       Save report to JSON file\n"
              << "  -h, --help              Show this help message\n";
}

int main(int argc, char** argv) {
    // 默认配置
    std::string model_path;
    std::string json_path;
    std::string opt_str = "all";
    bool probe_mode = false;
    BenchmarkConfig config;

    // 解析命令行参数
    struct option long_options[] = {
        {"model", required_argument, 0, 'm'},
        {"threads", required_argument, 0, 't'},
        {"requests", required_argument, 0, 'n'},
        {"warmup", required_argument, 0, 'w'},
        {"memory_limit", required_argument, 0, 'l'},
        {"optimization", required_argument, 0, 'o'},
        {"probe", no_argument, 0, 'p'},
        {"json", required_argument, 0, 'j'},
        {"help", no_argument, 0, 'h'},
        {0, 0, 0, 0}
    };

    int opt;
    int option_index = 0;
    while ((opt = getopt_long(argc, argv, "m:t:n:w:l:o:p:j:h", long_options, &option_index)) != -1) {
        switch (opt) {
            case 'm': model_path = optarg; break;
            case 't': config.threads = std::stoi(optarg); break;
            case 'n': config.requests = std::stoi(optarg); break;
            case 'w': config.warmup_rounds = std::stoi(optarg); break;
            case 'l': config.memory_limit_mb = std::stod(optarg); break;
            case 'o': opt_str = optarg; break;
            case 'p': probe_mode = true; break;
            case 'j': json_path = optarg; break;
            case 'h': PrintUsage(argv[0]); return 0;
            default: PrintUsage(argv[0]); return 1;
        }
    }

    // 校验必填参数
    if (model_path.empty()) {
        std::cerr << "Error: --model argument is required.\n";
        PrintUsage(argv[0]);
        return 1;
    }

    std::cout << "========================================" << std::endl;
    std::cout << " InferBench-Linux v0.1.0 (MVP) " << std::endl;
    std::cout << "========================================" << std::endl;
    std::cout << "Model: " << model_path << std::endl;
    std::cout << "Threads: " << config.threads << std::endl;
    std::cout << "Requests: " << config.requests << std::endl;
    std::cout << "Warmup: " << config.warmup_rounds << std::endl;
    std::cout << "----------------------------------------" << std::endl;

    try {
        // 1. 初始化模块
        std::cout << "[Init] Initializing Modules..." << std::endl;
        SystemMonitor monitor;
        InferenceEngine engine;

        // 2. 加载模型
        std::cout << "[Init] Loading Model..." << std::endl;
        
        int opt_level = 99; // Default all
        if (opt_str == "basic") opt_level = 1;
        else if (opt_str == "none") opt_level = 0;
        else if (opt_str == "all") opt_level = 99;
        else {
            std::cerr << "Warning: Unknown optimization level '" << opt_str << "', using 'all'." << std::endl;
        }

        engine.LoadModel(model_path, opt_level);

        // 如果是 Probe 模式，打印信息后退出
        if (probe_mode) {
           PrintModelInfo(engine);
           return 0;
        }

        // 3. 执行压测
        std::cout << "[Run] Starting Benchmark..." << std::endl;
        BenchmarkRunner runner(engine, monitor);
        BenchmarkResult result = runner.Run(config);

        // 4. 输出报告
        std::cout << "----------------------------------------" << std::endl;
        std::cout << " Benchmark Results " << std::endl;
        std::cout << "----------------------------------------" << std::endl;
        std::cout << std::fixed << std::setprecision(2);
        std::cout << "QPS:            " << result.qps << std::endl;
        std::cout << "Avg Latency:    " << result.avg_latency_ms << " ms" << std::endl;
        std::cout << "P99 Latency:    " << result.p99_latency_ms << " ms" << std::endl;
        std::cout << "Avg CPU Usage:  " << result.avg_cpu_usage << " %" << std::endl;
        std::cout << "Peak Memory:    " << result.peak_memory_mb << " MB" << std::endl;
        std::cout << "========================================" << std::endl;

        // 5. 保存 JSON
        if (!json_path.empty()) {
            std::ofstream json_file(json_path);
            if (json_file.is_open()) {
                json_file << "{\n";
                json_file << "  \"model\": \"" << model_path << "\",\n";
                json_file << "  \"config\": {\n";
                json_file << "    \"threads\": " << config.threads << ",\n";
                json_file << "    \"requests\": " << config.requests << "\n";
                json_file << "  },\n";
                json_file << "  \"result\": {\n";
                json_file << "    \"qps\": " << result.qps << ",\n";
                json_file << "    \"avg_latency_ms\": " << result.avg_latency_ms << ",\n";
                json_file << "    \"p99_latency_ms\": " << result.p99_latency_ms << ",\n";
                json_file << "    \"avg_cpu_usage\": " << result.avg_cpu_usage << ",\n";
                json_file << "    \"peak_memory_mb\": " << result.peak_memory_mb << "\n";
                json_file << "  }\n";
                json_file << "}\n";
                std::cout << "[Report] Saved to " << json_path << std::endl;
            } else {
                std::cerr << "[Error] Failed to save JSON report." << std::endl;
            }
        }

    } catch (const std::exception& e) {
        std::cerr << "\n[Fatal Error] " << e.what() << std::endl;
        return 1;
    }

    return 0;
}


==================================================
File: src/InferenceEngine.cpp
==================================================
#include "InferenceEngine.h"
#include <iostream>
#include <numeric>
#include <stdexcept>
#include <vector>
#include <cstring>

InferenceEngine::InferenceEngine() 
    : env_(ORT_LOGGING_LEVEL_WARNING, "InferBench") {
}

InferenceEngine::~InferenceEngine() {
    for (auto name : input_node_names_) {
        free(const_cast<char*>(name));
    }
    for (auto name : output_node_names_) {
        free(const_cast<char*>(name));
    }
}

void InferenceEngine::LoadModel(const std::string& model_path, int opt_level) {
    // Clear previous model resources if any
    for (auto name : input_node_names_) free(const_cast<char*>(name));
    for (auto name : output_node_names_) free(const_cast<char*>(name));
    input_node_names_.clear();
    output_node_names_.clear();
    input_node_dims_.clear();

    Ort::SessionOptions session_options;
    
    // Set Optimization Level
    switch (opt_level) {
        case 0:
            session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_DISABLE_ALL);
            break;
        case 1:
            session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_BASIC);
            break;
        case 2:
            session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_EXTENDED);
            break;
        case 99:
        default:
            session_options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
            break;
    }

    session_options.SetIntraOpNumThreads(1);

    try {
        session_ = std::make_unique<Ort::Session>(env_, model_path.c_str(), session_options);
    } catch (const Ort::Exception& e) {
        throw std::runtime_error("Failed to load model: " + std::string(e.what()));
    }

    Ort::AllocatorWithDefaultOptions allocator;

    // 处理输入节点
    size_t num_input_nodes = session_->GetInputCount();
    
    for(size_t i = 0; i < num_input_nodes; i++) {
        auto input_name_ptr = session_->GetInputNameAllocated(i, allocator);
        // 深拷贝 name，因为 API 返回的 smart pointer 会自动释放，而我们需要在 Run 中长期持有
        input_node_names_.push_back(strdup(input_name_ptr.get()));

        Ort::TypeInfo type_info = session_->GetInputTypeInfo(i);
        auto tensor_info = type_info.GetTensorTypeAndShapeInfo();
        
        input_node_dims_ = tensor_info.GetShape();
        
        // 处理动态维度 (Batch Size = -1)，强制设为 1
        for (auto& dim : input_node_dims_) {
            if (dim < 1) dim = 1;
        }

        input_tensor_size_ = 1;
        for (auto dim : input_node_dims_) {
            input_tensor_size_ *= dim;
        }
    }

    // 处理输出节点
    size_t num_output_nodes = session_->GetOutputCount();
    for(size_t i = 0; i < num_output_nodes; i++) {
        auto output_name_ptr = session_->GetOutputNameAllocated(i, allocator);
        output_node_names_.push_back(strdup(output_name_ptr.get()));
    }
}

std::vector<float> InferenceEngine::Run(const std::vector<float>& input_data) {
    if (input_data.size() != input_tensor_size_) {
        throw std::runtime_error("Input data size mismatch!");
    }

    auto memory_info = Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);

    // Zero-Copy 创建 input tensor
    Ort::Value input_tensor = Ort::Value::CreateTensor<float>(
        memory_info, 
        const_cast<float*>(input_data.data()), 
        input_data.size(), 
        input_node_dims_.data(), 
        input_node_dims_.size()
    );

    auto output_tensors = session_->Run(
        Ort::RunOptions{nullptr}, 
        input_node_names_.data(), 
        &input_tensor, 
        1,
        output_node_names_.data(), 
        1
    );

    float* floatarr = output_tensors[0].GetTensorMutableData<float>();
    auto output_tensor_info = output_tensors[0].GetTensorTypeAndShapeInfo();
    size_t output_size = output_tensor_info.GetElementCount();

    return std::vector<float>(floatarr, floatarr + output_size);
}

int64_t InferenceEngine::GetInputSize() const {
    return static_cast<int64_t>(input_tensor_size_);
}


==================================================
File: src/BenchmarkRunner.cpp
==================================================
#include "BenchmarkRunner.h"
#include <algorithm>
#include <atomic>
#include <chrono>
#include <iostream>
#include <random>
#include <thread>
#include <mutex>
#include <numeric>

BenchmarkRunner::BenchmarkRunner(InferenceEngine& engine, SystemMonitor& monitor)
    : engine_(engine), monitor_(monitor) {}

BenchmarkResult BenchmarkRunner::Run(const BenchmarkConfig& config) {
    BenchmarkResult result;

    // 1. 准备测试数据 (Random Input)
    int64_t input_size = engine_.GetInputSize();
    std::vector<float> input_data(input_size);
    std::mt19937 gen(42);
    std::uniform_real_distribution<float> dist(0.0f, 1.0f);
    for (auto& val : input_data) val = dist(gen);

    // 2. 预热 (Warmup)
    // 目的：让 CPU caches 热起来 & 触发 ONNX Runtime 内部可能的 JIT/Allocations
    for (int i = 0; i < config.warmup_rounds; ++i) {
        engine_.Run(input_data);
    }

    // 3. 准备并发控制
    std::atomic<int> remaining_requests(config.requests);
    std::vector<std::thread> threads;
    std::mutex stats_mutex; // 仅用于最后合并数据
    
    // Per-thread statistics to avoid lock verify
    std::vector<std::vector<double>> all_thread_latencies(config.threads);

    // 4. 启动系统监控线程
    std::atomic<bool> monitor_running(true);
    std::vector<double> cpu_samples;
    std::vector<double> mem_samples;
    
    std::thread monitor_thread([&]() {
        while (monitor_running) {
            double mem_usage = monitor_.GetMemoryUsage();
            
            double cpu = monitor_.GetCpuUsage();
            double mem = monitor_.GetMemoryUsage();
            
            cpu_samples.push_back(cpu);
            mem_samples.push_back(mem);

            // Watchdog Check
            if (config.memory_limit_mb > 0 && mem > config.memory_limit_mb) {
                std::cerr << "\n[Watchdog] OOM Detected! Current: " << mem 
                          << " MB > Limit: " << config.memory_limit_mb << " MB. Stopping..." << std::endl;
                monitor_running = false;
                // Force stop all requests
                remaining_requests = 0;
            }

            std::this_thread::sleep_for(std::chrono::milliseconds(100));
        }
    });

    // 5. 启动 Worker 线程
    auto start_time = std::chrono::high_resolution_clock::now();

    for (int t = 0; t < config.threads; ++t) {
        threads.emplace_back([&, t]() {
            while (true) {
                // 抢单：原子减
                // fetch_sub 返回修改前的值
                int current_req_idx = remaining_requests.fetch_sub(1);
                if (current_req_idx <= 0) {
                    break; // 抢没了，下班
                }

                auto t1 = std::chrono::high_resolution_clock::now();
                engine_.Run(input_data);
                auto t2 = std::chrono::high_resolution_clock::now();

                // 记录延迟 (毫秒)
                double lat_ms = std::chrono::duration<double, std::milli>(t2 - t1).count();
                all_thread_latencies[t].push_back(lat_ms);
            }
        });
    }

    // 6. 等待所有 Worker 结束
    for (auto& t : threads) {
        if (t.joinable()) t.join();
    }

    auto end_time = std::chrono::high_resolution_clock::now();
    double total_time_sec = std::chrono::duration<double>(end_time - start_time).count();

    // 7. 停止监控
    monitor_running = false;
    if (monitor_thread.joinable()) monitor_thread.join();

    // 8. 汇总数据
    // 合并 Latency
    std::vector<double> flat_latencies;
    flat_latencies.reserve(config.requests);
    for (const auto& local_lats : all_thread_latencies) {
        flat_latencies.insert(flat_latencies.end(), local_lats.begin(), local_lats.end());
    }

    // 计算统计指标
    result.qps = config.requests / total_time_sec;
    
    if (!flat_latencies.empty()) {
        double sum = std::accumulate(flat_latencies.begin(), flat_latencies.end(), 0.0);
        result.avg_latency_ms = sum / flat_latencies.size();
        result.p99_latency_ms = CalculatePercentile(flat_latencies, 0.99);
    }

    if (!cpu_samples.empty()) {
        double sum = std::accumulate(cpu_samples.begin(), cpu_samples.end(), 0.0);
        result.avg_cpu_usage = sum / cpu_samples.size();
    }

    if (!mem_samples.empty()) {
        // 峰值内存
        result.peak_memory_mb = *std::max_element(mem_samples.begin(), mem_samples.end());
    }

    return result;
}

double BenchmarkRunner::CalculatePercentile(std::vector<double>& latencies, double percentile) {
    if (latencies.empty()) return 0.0;
    
    // 排序以找到 P99
    std::sort(latencies.begin(), latencies.end());
    
    // index = ceil(p / 100 * N) - 1
    // 或简单的 index = p * N
    size_t index = static_cast<size_t>(percentile * latencies.size());
    
    // 边界保护
    if (index >= latencies.size()) index = latencies.size() - 1;
    
    return latencies[index];
}


==================================================
File: src/SystemMonitor.cpp
==================================================
#include "SystemMonitor.h"
#include <fstream>
#include <sstream>
#include <iostream>
#include <unistd.h>

double SystemMonitor::GetMemoryUsage() {
    // 打开当前进程的状态文件 /proc/self/stat
    std::ifstream stat_file("/proc/self/stat");
    if (!stat_file.is_open()) {
        return 0.0;
    }

    std::string field;
    // /proc/[pid]/stat 文件中第 24 个字段是 rss (Resident Set Size)
    // 它表示进程当前占用的物理内存页数。我们需要跳过前 23 个字段。
    for (int i = 0; i < 24; ++i) {
        if (!(stat_file >> field)) {
            return 0.0;
        }
    }

    // 将第 24 个字段（rss 页数）转换为长整型
    long rss = std::stol(field);
    // 获取系统的内存页大小（通过 sysconf 获取，通常为 4096 字节）
    long page_size = sysconf(_SC_PAGESIZE);
    
    // 计算总内存使用量（字节）并转换为 MB
    // 计算公式：(页数 * 每页字节数) / (1024 * 1024)
    return (static_cast<double>(rss) * page_size) / (1024.0 * 1024.0);
}

double SystemMonitor::GetCpuUsage() {
    // 加锁，防止多线程同时修改 last_total_cpu_time_ 等状态
    std::lock_guard<std::mutex> lock(mutex_);

    // 1. 打开 /proc/stat 文件，这是全局 CPU 状态
    std::ifstream stat_file("/proc/stat");
    if (!stat_file.is_open()) {
        return 0.0;
    }

    std::string line;
    std::getline(stat_file, line); // 读取第一行，通常以 "cpu " 开头，代表总 CPU

    std::istringstream iss(line);
    std::string cpu_label;
    iss >> cpu_label; // 跳过 "cpu" 字符串

    // 2. 读取各个时间片数据
    // Linux 内核文档定义了这些字段顺序：user, nice, system, idle, iowait, irq, softirq, steal, guest, guest_nice
    int64_t user, nice, system, idle, iowait, irq, softirq, steal;
    
    // 只读取前 8 个字段通常足够计算
    if (!(iss >> user >> nice >> system >> idle >> iowait >> irq >> softirq >> steal)) {
        return 0.0;
    }

    // 3. 计算 Total 和 Idle
    // Idle 时间 = idle + iowait
    // Total 时间 = user + nice + system + idle + iowait + irq + softirq + steal
    int64_t current_idle = idle + iowait;
    int64_t current_total = user + nice + system + idle + iowait + irq + softirq + steal;

    // 4. 计算与上一次读数的差值 (Delta)
    int64_t total_delta = current_total - last_total_cpu_time_;
    int64_t idle_delta = current_idle - last_idle_cpu_time_;

    // 5. 更新状态以备下次使用
    last_total_cpu_time_ = current_total;
    last_idle_cpu_time_ = current_idle;

    // 防止除以零（通常也是处理第一次运行的情况）
    if (total_delta <= 0) {
        return 0.0;
    }

    // 6. 计算使用率
    // 这里的逻辑反向思考：使用率 = 1.0 - 空闲率
    double usage = 100.0 * (1.0 - static_cast<double>(idle_delta) / static_cast<double>(total_delta));
    return usage < 0 ? 0 : usage;
}

bool SystemMonitor::CheckMemoryLimit(double limit_mb) {
    if (limit_mb <= 0) {
        return false; // 0 or negative means no limit
    }
    double current_usage = GetMemoryUsage();
    return current_usage > limit_mb;
}


==================================================
File: include/InferenceEngine.h
==================================================
#pragma once

#include <string>
#include <vector>
#include <memory>
#include <onnxruntime_cxx_api.h>

/**
 * @brief 推理引擎类，封装 ONNX Runtime 的核心功能。
 * 
 * 负责模型的加载、Tensor 内存管理以及执行推理。
 */
class InferenceEngine {
public:
    InferenceEngine();
    ~InferenceEngine();

    /**
     * @brief 加载 ONNX 模型。
     * 
     * @param model_path 模型文件的路径 (.onnx)。
     * @param opt_level 优化级别 (0=Disable, 1=Basic, 99=All). Default: 99.
     * @throws std::runtime_error 如果加载失败。
     */
    void LoadModel(const std::string& model_path, int opt_level = 99);

    /**
     * @brief 执行推理。
     * 
     * @param input_data 输入数据（展平的 float 数组）。
     * @return std::vector<float> 推理结果（展平的 float 数组）。
     */
    std::vector<float> Run(const std::vector<float>& input_data);

    /**
     * @brief 获取模型需要的输入 Tensor 元素总数。
     * 
     * 用于生成符合大小的随机测试数据。
     * 
     * @return int64_t 元素个数 (如 1x3x224x224 = 150528)。
     */
    int64_t GetInputSize() const;

private:
    // ONNX Runtime 环境，整个进程通常只需要一个
    Ort::Env env_;
    // 会话对象，非线程安全（但在 Run 时只读模型是安全的，如果包含状态则需注意）
    // 为了更安全的并发，通常 session 是线程安全的，但 session options 不是。
    std::unique_ptr<Ort::Session> session_;
    
    // 模型的元数据 (Metadata)
    std::vector<const char*> input_node_names_;
    std::vector<const char*> output_node_names_;
    std::vector<int64_t> input_node_dims_;
    size_t input_tensor_size_ = 0;
};


==================================================
File: include/BenchmarkRunner.h
==================================================
#pragma once

#include "InferenceEngine.h"
#include "SystemMonitor.h"
#include <vector>
#include <cstdint>

/**
 * @brief 压测配置参数
 */
struct BenchmarkConfig {
    int threads = 1;        ///< 并发线程数
    int requests = 100;     ///< 总请求数
    int warmup_rounds = 10; ///< 预热轮数（不计入统计）
    double memory_limit_mb = 0.0; ///< 内存限制 (MB)，0 表示不限制
};

/**
 * @brief 压测结果统计
 */
struct BenchmarkResult {
    double qps = 0.0;            ///< 吞吐量 (Queries Per Second)
    double avg_latency_ms = 0.0; ///< 平均延迟 (毫秒)
    double p99_latency_ms = 0.0; ///< P99 延迟 (毫秒)
    double avg_cpu_usage = 0.0;  ///< 平均 CPU 使用率 (%)
    double peak_memory_mb = 0.0; ///< 峰值内存占用 (MB)
};

/**
 * @brief 压测核心调度器
 * 
 * 负责管理线程池、分发推理任务、收集性能数据以及控制系统监控。
 * 采用“抢单模式”进行负载均衡。
 */
class BenchmarkRunner {
public:
    /**
     * @brief 构造函数
     * 
     * @param engine 依赖的推理引擎 (引用，生命周期需长于 Runner)
     * @param monitor 依赖的系统监控器 (引用，生命周期需长于 Runner)
     */
    BenchmarkRunner(InferenceEngine& engine, SystemMonitor& monitor);
    ~BenchmarkRunner() = default;

    /**
     * @brief 执行压测
     * 
     * 此函数是阻塞的，直到所有请求完成。
     * 
     * @param config 压测配置
     * @return BenchmarkResult 最终统计结果
     */
    BenchmarkResult Run(const BenchmarkConfig& config);

private:
    InferenceEngine& engine_;
    SystemMonitor& monitor_;

    /**
     * @brief 计算延迟的百分位数
     * 
     * @param latencies 所有样本数据
     * @param percentile 百分位 (e.g. 0.99)
     * @return double 对应的延迟值
     */
    double CalculatePercentile(std::vector<double>& latencies, double percentile);
};


==================================================
File: include/SystemMonitor.h
==================================================
#pragma once

#include <string>
#include <cstdint>
#include <mutex>

/**
 * @brief 负责监控系统资源（CPU 和 内存）
 * 
 * 这是一个工具类，直接读取 Linux 的 /proc 文件系统。
 */
class SystemMonitor {
public:
    SystemMonitor() = default;
    ~SystemMonitor() = default;

    /**
     * @brief 获取当前进程的物理内存占用 (RSS)
     * @return 占用内存大小，单位：MB
     */
    double GetMemoryUsage();

    /**
     * @brief 获取系统的整体 CPU 使用率
     * 
     * 注意：这是一个瞬时值计算。
     * 由于 /proc/stat 存储的是累加值，我们需要维护上一次的读数，
     * 计算 (Current - Last) 的差值来得出这段时间内的使用率。
     * 
     * @return CPU 使用率百分比 (0.0 - 100.0)
     */
    double GetCpuUsage();

    /**
     * @brief 检查内存是否超过限制 (资源熔断)
     * 
     * @param limit_mb 内存限制阈值 (MB)
     * @return true 如果当前 RSS 超过 limit_mb
     * @return false 如果未超过或 limit_mb <= 0
     */
    bool CheckMemoryLimit(double limit_mb);

private:
    // 记录上一次读取的 CPU 时间片总和
    int64_t last_total_cpu_time_ = 0;
    // 记录上一次读取的 CPU 空闲时间
    int64_t last_idle_cpu_time_ = 0;
    // 互斥锁，保证多线程访问安全
    std::mutex mutex_;
};


==================================================
File: scripts/benchmark_suite.py
==================================================
#!/usr/bin/env python3
import subprocess
import json
import matplotlib.pyplot as plt
import os
import argparse
import sys

def run_benchmark(executable, model, threads, requests, warmup):
    """Run inferbench and return the result dictionary."""
    output_json = f"result_t{threads}.json"
    cmd = [
        executable,
        "--model", model,
        "--threads", str(threads),
        "--requests", str(requests),
        "--warmup", str(warmup),
        "--json", output_json,
        "--optimization", "all" # Ensure best performance
    ]
    
    print(f"Running: {' '.join(cmd)}")
    try:
        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    except subprocess.CalledProcessError as e:
        print(f"Error running benchmark: {e.stderr.decode()}")
        return None

    if not os.path.exists(output_json):
        print(f"Error: {output_json} not generated.")
        return None

    with open(output_json, 'r') as f:
        data = json.load(f)
    
    # Cleanup
    os.remove(output_json)
    return data['result']

def plot_results(threads_list, qps_list, lat_list, output_dir):
    """Generate QPS and Latency plots."""
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    # 1. Throughput (QPS)
    plt.figure(figsize=(10, 6))
    plt.plot(threads_list, qps_list, marker='o', linewidth=2, color='#28a745')
    plt.title('Throughput vs Threads')
    plt.xlabel('Threads')
    plt.ylabel('QPS (Queries Per Second)')
    plt.grid(True, linestyle='--', alpha=0.7)
    for x, y in zip(threads_list, qps_list):
        plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')
    plt.savefig(os.path.join(output_dir, 'throughput.png'))
    plt.close()

    # 2. Latency
    plt.figure(figsize=(10, 6))
    plt.plot(threads_list, lat_list, marker='s', linewidth=2, color='#dc3545')
    plt.title('Average Latency vs Threads')
    plt.xlabel('Threads')
    plt.ylabel('Latency (ms)')
    plt.grid(True, linestyle='--', alpha=0.7)
    for x, y in zip(threads_list, lat_list):
        plt.text(x, y, f'{y:.1f}', ha='center', va='bottom')
    plt.savefig(os.path.join(output_dir, 'latency.png'))
    plt.close()
    
    print(f"Plots saved to {output_dir}/")

def main():
    parser = argparse.ArgumentParser(description="InferBench Automation Suite")
    parser.add_argument("--bin", default="./build/bin/inferbench", help="Path to inferbench executable")
    parser.add_argument("--model", required=True, help="Path to ONNX model")
    parser.add_argument("--requests", type=int, default=100, help="Requests per thread")
    args = parser.parse_args()

    if not os.path.exists(args.bin):
        print(f"Error: Executable {args.bin} not found.")
        sys.exit(1)
        
    if not os.path.exists(args.model):
        print(f"Error: Model {args.model} not found.")
        sys.exit(1)

    threads_config = [1, 2, 4, 8, 16]
    results_qps = []
    results_lat = []

    print("=== Starting Benchmark Suite ===")
    
    for t in threads_config:
        # Scale requests with threads to keep load sufficient, or keep per-thread constant?
        # Runner logic: total requests are distributed.
        # Let's keep total requests proportional to threads to ensure longer running time?
        # Or simple constant total requests? 
        # For meaningful throughput test, total requests should be large.
        # Let's use user arg --requests as "requests per thread" roughly
        total_reqs = args.requests * t
        
        res = run_benchmark(args.bin, args.model, t, total_reqs, warmup=10)
        if res:
            results_qps.append(res['qps'])
            results_lat.append(res['avg_latency_ms'])
            print(f"  [Threads: {t}] QPS: {res['qps']:.2f}, Latency: {res['avg_latency_ms']:.2f} ms")
        else:
            print(f"  [Threads: {t}] Failed")
            results_qps.append(0)
            results_lat.append(0)

    plot_results(threads_config, results_qps, results_lat, ".")

if __name__ == "__main__":
    main()


==================================================
File: scripts/mem_check.sh
==================================================
#!/bin/bash
set -e

echo "=== Memory Safety Check (ASan) ==="

# 1. Clean and Rebuild with ASan
echo "[Build] Compiling with AddressSanitizer..."
rm -rf build_asan
mkdir -p build_asan
cd build_asan
cmake -DCMAKE_BUILD_TYPE=Debug -DENABLE_ASAN=ON ..
make -j$(nproc)

# 2. Run Unit Tests
echo "[Test] Running Unit Tests..."
./bin/unit_tests

# 3. Run Benchmark (Integration Test)
# Use a small number of requests just to check for leaks during execution
echo "[Test] Running InferenceEngine Integration..."
# Assuming we have the test model available
MODEL_PATH="../tests/resnet50.onnx"
if [ ! -f "$MODEL_PATH" ]; then
    echo "Warning: Test model not found at $MODEL_PATH, skipping integration run."
else
    # Run with leak check enabled (ASan default)
    # We add export ASAN_OPTIONS=detect_leaks=1 just to be explicit, though usually default on Linux x86_64
    export ASAN_OPTIONS=detect_leaks=1:symbolize=1
    ./bin/inferbench --model "$MODEL_PATH" --requests 10 --threads 2 --optimization basic
fi

echo "=== Memory Check Passed! ==="


==================================================
File: cmake/arm-linux.toolchain.cmake
==================================================
# CMake Cross-Compilation Toolchain for ARM Linux
# Usage: cmake -DCMAKE_TOOLCHAIN_FILE=../cmake/arm-linux.toolchain.cmake ..

set(CMAKE_SYSTEM_NAME Linux)
set(CMAKE_SYSTEM_PROCESSOR aarch64)

# Set the cross compiler
# Ensure these are in your PATH or set absolute paths
set(CMAKE_C_COMPILER aarch64-linux-gnu-gcc)
set(CMAKE_CXX_COMPILER aarch64-linux-gnu-g++)

# Target environment root
set(CMAKE_FIND_ROOT_PATH /usr/aarch64-linux-gnu)

# Search for programs in the build host directories
set(CMAKE_FIND_ROOT_PATH_MODE_PROGRAM NEVER)

# Search for libraries and headers in the target directories
set(CMAKE_FIND_ROOT_PATH_MODE_LIBRARY ONLY)
set(CMAKE_FIND_ROOT_PATH_MODE_INCLUDE ONLY)

# Optimization flags for ARM64
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -march=armv8-a -mtune=cortex-a53")


==================================================
File: .github/workflows/ci.yml
==================================================
name: InferBench CI

on:
  push:
    branches: [ "main", "feature/*" ]
  pull_request:
    branches: [ "main" ]

jobs:
  build-and-test:
    runs-on: ubuntu-22.04

    steps:
    - uses: actions/checkout@v3

    - name: Install Dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y python3-pip python3-matplotlib

    - name: Download ONNX Runtime
      run: |
        # Download prebuilt Linux x64 ONNX Runtime
        wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.3/onnxruntime-linux-x64-1.16.3.tgz
        tar xzf onnxruntime-linux-x64-1.16.3.tgz
        mkdir -p third_party/onnxruntime
        cp -r onnxruntime-linux-x64-1.16.3/* third_party/onnxruntime/

    - name: Configure CMake
      run: cmake -S . -B build -DCMAKE_BUILD_TYPE=Release

    - name: Build
      run: cmake --build build

    - name: Download Test Model
      run: |
        mkdir -p tests
        # Download a lightweight SqueezeNet model and rename it to match the test expectation
        wget -q https://github.com/onnx/models/raw/main/validated/vision/classification/squeezenet/model/squeezenet1.0-9.onnx -O tests/resnet50.onnx

    - name: Run Units Tests
      run: |
        cd build
        ctest --output-on-failure
        
    - name: Test Watchdog (Memory Limit)
      run: |
        # Just ensure binary runs and argument works
        ./build/bin/inferbench --help | grep memory_limit

    - name: Check F3 Automation Script Syntax
      run: |
        python3 -m py_compile scripts/benchmark_suite.py

