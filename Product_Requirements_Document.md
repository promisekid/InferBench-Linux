# 产品需求文档 (PRD): InferBench-Linux
> **版本**: v0.1.0 (MVP)  
> **日期**: 2026-01-17  
> **状态**: 已批准  

## 1. 项目背景
### 1.1 痛点分析
在 AI 算法落地过程中，除了模型精度，推理延迟 (Latency)、吞吐量 (QPS) 以及系统资源占用 (CPU/Memory) 是决定模型能否上线的关键指标。目前缺乏一个轻量级、无依赖、且能同时监控系统底层指标的 C++ 原生测试工具。

### 1.2 项目目标
开发一个运行在 Linux 终端的命令行工具 (CLI) `inferbench`，具备以下能力：
- 加载通用的 ONNX 模型（避免引入 OpenCV 等复杂依赖）。
- 进行多线程高并发推理压测。
- 实时监控 Linux 系统级（整体 CPU）和进程级（当前进程内存）的资源消耗。
- 输出标准化的性能测试报告 (JSON/CSV)。

## 2. 技术架构
### 2.1 技术栈
- **核心语言**: C++17
- **构建系统**: CMake
- **推理后端**: ONNX Runtime (C++ API)
- **并发模型**: `std::thread` + 线程池
- **系统监控**: 直接读取 Linux `/proc` 文件系统 (无第三方依赖)
- **单元测试**: Google Test (GTest)

### 2.2 模块划分
1.  **接入层 (Interface Layer)**: 负责解析命令行参数 (`--model`, `--threads` 等) 及流程控制。
2.  **推理引擎 (InferenceEngine)**: 封装 ONNX Runtime 会话，负责模型加载和 Tensor 管理。
3.  **压测核心 (BenchmarkRunner)**: 负责线程池管理、任务调度和延迟计算。
4.  **监控模块 (SystemMonitor)**: 后台线程读取 `/proc/stat` 和 `/proc/self/stat`。
5.  **数据层 (Data Layer)**: 负责统计数据聚合及报告生成。

## 3. MVP 功能需求

### 3.1 模型与数据
- **模型加载**: 支持通过文件路径加载 `.onnx` 模型。
- **数据输入**:
    - 自动探测输入节点的名称和维度。
    - 自动生成符合 Input Shape 的随机浮点张量 (Float32)。
    - *约束*: MVP 阶段不引入图像处理库（如 OpenCV）。

### 3.2 压测配置
- **并发控制**: 支持配置线程数 (`-t/--threads`)。
- **负载控制**: 
    - 固定请求总数 (`-n/--requests`)。
    - *优化项*: 支持预热轮数 (`--warmup`) 以稳定性能。

### 3.3 系统监控
- **采样频率**: 可配置（默认如 100ms）。
- **监控指标**:
    - **进程内存**: RSS (常驻内存集)。
    - **系统 CPU**: 全局 CPU 使用率百分比。

### 3.4 输出与报告
- **交互体验**: 终端实时显示进度条。
- **结果汇总**:
    - 平均延迟 (Avg Latency)、P99 延迟。
    - 吞吐量 (QPS)。
    - 峰值内存 (Peak Memory RSS)。
    - 平均 CPU 使用率。
- **文件输出**: 标准化 JSON 和 CSV 报告。

## 4. 工作流程
1.  **用户** 启动工具: `./inferbench --model resnet50.onnx --threads 4 --requests 1000`
2.  **工具** 初始化环境 (ONNX Env) -> 加载模型。
3.  **工具** 执行预热 (Warmup, 可选)。
4.  **工具** 启动监控线程 (Monitor Thread)。
5.  **工具** 启动工作线程池 (Benchmark Work)。
6.  **工具** 聚合结果 -> 打印到终端 -> 保存至文件。
